---
title: 'Score Matching'
date: 2025-10-28
permalink: /posts/2025/10/blog-post-2/
tags:
  - Score Matching
  - 分数匹配
  - Denoising Score Matching
  - Sliced Score Matching
  - Langevin MCMC
  - 朗之万动力学
---

Score Matching（得分匹配）是一种不依赖分布概率的归一化常数的参数估计方法，核心思想是：直接让模型输出“对数概率密度函数的梯度（即：`∇_x log(p(x))`）”，去逼近真实数据分布的得分函数，而不是像最大似然那样去拟合概率值本身。它的主要优点是无需计算概率的归一化常数，与扩散/能量模型天然契合。

相关论文：
* [Estimation of Non-Normalized Statistical Models by Score Matching](https://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf)
* [A Connection Between Score Matching and Denoising Autoencoders](https://gwern.net/doc/ai/nn/diffusion/2011-vincent.pdf)
* [Sliced Score Matching: A Scalable Approach to Density and Score Estimation](https://arxiv.org/abs/1905.07088v2)

---

## Score Matching
### 什么是 Score ？
定义：Score 是对数概率密度函数对输入的梯度，也称为得分函数（Score Function）:
$$
\mathbf{s}(\mathbf{x}) = \nabla_{\mathbf{x}} \log p(\mathbf{x}) \tag{1}
$$

### 不需要计算归一化常数（partition function）
假设由能量函数 `E(x;θ)` 计算概率分布 `p(x;θ)` ，我们需要计算归一化常数（即配分项，它是关于 `x` 的常数） `Z(θ)` ：
$$
p(\mathbf{x};θ) = \frac{1}{Z(θ)}e^{-E(\mathbf{x};θ)}
$$

由公式（1），显然有：
$$
\mathbf{s}(\mathbf{x}) = \nabla_{\mathbf{x}} \log p(\mathbf{x}) = -\nabla_{\mathbf{x}}E(x;θ)
$$
从而消除配分项 `Z(θ)` 。

#### Score Matching 的目标
最小化模型 score 与真实 score 之间的平方误差 ：
$$
J(\mathbf{\theta}) = \frac{1}{2} \mathbb{E}_{p_{\text{data}}} \left[ \| \mathbf{s}_{\mathbf{\theta}}(\mathbf{x}) - \mathbf{s}_{\text{data}}(\mathbf{x}) \|^2 \right] \tag{2}
$$
其中， `s_θ(x)` 返回模型关于 `x` 的 score ， `s_data(x)` 返回真实数据分布关于 `x` 的 score 。

然而， `s_data(x)` 通常难以计算，论文[Estimation of Non-Normalized Statistical Models by Score Matching](https://jmlr.org/papers/volume6/hyvarinen05a/hyvarinen05a.pdf)证明了，公式（2）等价于以下形式：
$$
J(\theta) = E_{p_{data}}\left[ tr [\nabla_x \mathbf{s}(\mathbf{x};\theta)] + \frac{1}{2} \lVert \mathbf{s}(\mathbf{x};\theta) \rVert^2 \right] \tag{3}
$$
这样，只需要从真实分布采样，无需计算真实分布的得分函数。

### 基于朗之万动力学（Langevin Dynamics）生成
使用公式（3）训练得到分数函数后，一般采用朗之万动力学的离散时间形式进行生成采样：
$$
\mathbf{x}_{k+1} = \mathbf{x}_k + η \mathbf{s}_\theta(\mathbf{x}_k) + \sqrt{2η}\mathbf{z_t} , z_t \in N(0,1)  \tag{4}
$$
其中， `x_0` 可以从取任意值，一般从均匀分布或高斯分布中采样。

可以证明，只要步长 `η` 要足够小，总步数 `K` 足够大；`x` 就会收敛到目标分布 `p_x` 上。并且，`p(x)` 是平稳解；在 mild 条件下（`p` 严格正、光滑、可积）还是唯一平稳解。

这个方法的核心是：**沿着 score 方向移动（第2项） + 随机游走（第3项）**
前者保证了 `x` 逐步靠近目标分布 `p_x` 的高概率密度的“峰”。
后者保证了 `x` 不会永远困在某个高概率密度的“峰”上。

但是，也要注意，对于概率空间中的“山峰”，依赖随机游走的力量离开它，需要指数级的时间。一般可以通过模拟退火等方式减轻这个问题。

## Denoising Score Matching

### 问题背景
高维数据中，数据可能位于低维流形上，在流形以外的空间上概率密度为0，从而分数函数无定义（因为它是 `log-pdf` 的梯度，`log0` 无意义）。

### 核心思想
引入噪声扰动数据：`x = x + ϵ` ， `ϵ ∼ N(0, σ)`，让模型学习被扰动后的数据的 score 。 由于正态分布是全局非0的，从而解决梯度消失问题。

我们用 \\( \bar{x} \\) 表示扰动后的数据分布，显然：
$$
\begin{align}
p(\bar{x}|x) &= N(x, σ^2I) \\
\frac{\partial \log p(\bar{x}|x)}{\partial \bar{x}} &= \frac{x-\bar{x}}{\sigma^2}
\end{align}
$$

### 优化目标
由分数函数定义可知：
$$
J_{ESM}(\mathbf{\theta}) = \frac{1}{2} \mathbb{E}_{\bar{x} \sim p(\bar{x})} \left[ \| \mathbf{s}(\bar{x}; \theta) - \frac{\partial \log p(\bar{x})}{\partial \bar{x}} \|^2 \right] \tag{5}
$$

DSM 的目标函数如下：
$$
\begin{align}
J(\mathbf{\theta})
&= \frac{1}{2} \mathbb{E}_{x \sim p(x),\bar{x} \sim p(\bar{x}|x)} \left[ \| \mathbf{s}(\bar{x}; \theta) - \frac{\partial \log p(\bar{x}|x)}{\partial \bar{x}} \|^2 \right] \\
&= \frac{1}{2} \mathbb{E}_{x \sim p(x),\epsilon \sim N(0, \sigma^2 I)} \left[ \| \mathbf{s}(x+\epsilon; \theta) + \frac{\epsilon}{\sigma^2} \|^2 \right] \tag{6} \\
\end{align}
$$

[A Connection Between Score Matching and Denoising Autoencoders](https://gwern.net/doc/ai/nn/diffusion/2011-vincent.pdf) 证明了优化公式（6）的 DSM 目标，等价于优化公式（5）的 ESM 的目标。

公式（6）也说明，模型 `s(⋅;θ)` 学习的是其输入数据中添加的随机噪声 `ϵ` 的反方向（即去噪）。

## Sliced Score Matching
这是对 Score Matching 在数值计算上的一种优化，具体来说，通过将模型和数据的分数向量映射到随机方向上，再对结果（1维数据）求MSE作为损失。

其优化目标如下：
$$
\begin{align}
L(\theta ; p_v)
& \triangleq \frac{1}{2} \mathbb{E}_{p_v} \mathbb{E}_{p_d}\left[\left(v^{\top} \mathbf{s}_m(x ; \theta)-v^{\top} \mathbf{s}_d(x)\right)^2\right] \\
& \triangleq \mathbb{E}_{p_v} \mathbb{E}_{p_d}\left[ v^\top \nabla_x \mathbf{s}_m(x;\theta)v + \frac{1}{2}\left(v^\top \mathbf{s}_m(x;\theta) \right)^2 \right] \\
& \triangleq \mathbb{E}_{p_v} \mathbb{E}_{p_d}\left[ v^\top \nabla_x \left(v^\top \mathbf{s}_m(x;\theta)\right) + \frac{1}{2}\| \mathbf{s}_m(x;\theta) \|^2 \right] \tag{7}
\end{align}
$$

其中， `p_v` 为随机向量 `v` 的概率分布， `s_m` 为模型分数函数， `s_d` 数据分数函数。对于 `p_v` ，需要满足 `E[v*v_T] = I` 。

观察公式（7），注意到第一项中梯度运算的对象 \\( v^\top \mathbf{s}_m(x;\theta) \\) 是一个标量，这意味只需要进行一次反向传播，从而简化了训练。
