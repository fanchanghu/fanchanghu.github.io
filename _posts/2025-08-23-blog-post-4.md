---
title: 'Continuous Normalizing Flow（CNF）'
date: 2025-08-23
permalink: /posts/2025/08/blog-post-4/
tags:
  - Normalizing Flows
  - Continuous Normalizing Flow
  - CNF
  - Neural ODE
---

Continuous Normalizing Flow（CNF）是 [Neural ODE](https://arxiv.org/abs/1806.07366v5) 中提出的一种对传统离散 [Normalizing Flow（NF）](https://arxiv.org/abs/1505.05770v6) 的改进版本。它将 NF 中的离散流（K 层可逆映射），改造为连续的 Neural ODE 网络，降低了运算复杂度，从而能够支持“宽流（并行多个流）”，并提升流的拟合能力。

---

### CNF 的核心思想

Normalizing Flow 的核心思想依赖如下公式：

$$
z_k = f(z_{k-1}) \implies
\log p(z_k) = \log p(z_{k-1}) - \log \vert \det \frac{\partial f_k}{\partial z_{k-1}} \vert \tag{1}
$$

通过 K 层可逆映射 f，把分布从简单分布 z_0，转变为复杂的后验分布 z_K。

公式（1）与残差网络类似，可以使用 Neural ODE 改造为连续版本，如下：

$$
\frac{dz}{dt} = f(z(t), t) \implies
\frac{\partial p(z(t))}{\partial t} = -tr \left( \frac{df}{dz(t)} \right) \tag{2}
$$

公式（2）与（1）式对比，将行列式降为求迹运算。由于 \\( tr(\sum J_n) = \sum tr(J_n) \\) ，我们可以以 **O(M)** 的复杂度并行多个隐藏单元（hidden-unit），其中 σ(t) 是由神经网络学习的权重系数：

$$
\frac{dz}{dt} = \sum_{n=1}^M \sigma_n(t)f_n(z(t), t) \implies
\frac{\partial p(z(t))}{\partial t} = -\sum_{n=1}^M \sigma_n(t) tr(\frac{df_n}{dz(t)}) \tag{3}
$$

而离散流如果要并行，需要的时间复杂度是 **O(M^3)** 。

### CNF-VAE 网络架构

![Illustration Variational Inference with CNF](/images/202508/cnf-1.png)
图1

与离散的 NF 类似（参考 [归一化流学习](/posts/2025/08/blog-post-2/) 中的图2），CNF 的训练也可以归纳为“**编码 → 流动 → 解码 → 优化**”：
1. **编码器**：输入观测 x，输出初始潜变量参数 q₀ = N(μ, Σ)
2. **流动**：从 \\( q_0(z_0 \mid x) \\) 采样 z₀，利用公式（3）求解 ODESolver 得到 z。CNF 中的相关参数记为 ψ 。
3. **解码器**：用 z 计算 \\( \log p_\theta(x \mid z) \\) ，而先验分布 p(z) 通常取标准高斯。
4. **优化**：Loss 为负 ELBO 。

### 实验结果

Neural ODE 中没有直接关于 CNF-VAE 的实验，作者通过最小化 KL 散度，比较了 CNF和 NF 的对不同分布的拟合能力，CNF 取得了更小的KL值。如下图：

![Illustration NF vs CNF about KL](/images/202508/cnf-2.png)
图2
