---
title: 'GRPO学习'
date: 2025-06-23
permalink: /posts/2025/06/blog-post-1/
tags:
  - LLM
  - RL
  - GRPO
  - PPO
---

该算法由DeepSeek在[DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300v3)中提出。

该算法是对RLHF中PPO算法的优化，通过优化\\( A_t \\)的计算方法，无需使用价值网络，从而减少了训练过程的开销。
GRPO的主要过程如下图：

作者首先分析了PPO的缺点：
1. 需要同时学习策略网络和价值网络。价值网络的参数规模与策略网络相当，这增加了训练开销；
2. 由于RLHF中的奖励是稀疏的（只在最后一步才能得到奖励），导致价值网络训练不稳定。价值函数在优势估计中相当于PO算法的基线，用于减少方差，价值函数如果不稳定，可能难以发挥作用，从而导致RL过程不稳定。

为了解决上述问题，GRPO提出了一种新的动作优势估计方法：对于每个问题q，从策略模型采样得到多个回答，并使用奖励模型计算回报 \\( r = {𝑟_1,𝑟_2, ··· ,𝑟_𝐺} \\) ，然后对上述回报进行正则化，使其均值为0方差为1： \\( \hat A_{i, t} = \overset{-}{r}_i = \frac{r_i - mean(\mathbf{r})}{std(\mathbf{r})} \\) ，作为当前回答中每个token的优势。

此外，为了保证策略模型不偏离参考模型太远，RLHF使用PPO时，在奖励中添加了策略模型和参考模型的KL散度。而GRPO则将策略模型和参考模型的KL散度直接添加在优化目标中。

最终，GRPO的优化目标如下：
$$
\mathcal{J}_{G R P O}(\theta) =\mathbb{E}_{q \sim P(Q),\left\{o_{i}\right\}_{i=1}^{G} \sim \pi_{\theta_{\text {old }}}(O \mid q)} \frac{1}{G} \sum_{i=1}^{G} \frac{1}{\left|o_{i}\right|} \sum_{t=1}^{\left|o_{i}\right|}\left\{\min \left[\frac{\pi_{\theta}\left(o_{i, t} \mid q, o_{i,<t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,<t}\right)} \hat{A}_{i, t}, \operatorname{clip}\left(\frac{\pi_{\theta}\left(o_{i, t} \mid q, o_{i,<t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{i, t} \mid q, o_{i,<t}\right)}, 1-\varepsilon, 1+\varepsilon\right) \hat{A}_{i, t}\right]-\beta \mathbb{D}_{K L}\left[\pi_{\theta}| | \pi_{r e f}\right]\right\}
$$

作为对比，PPO优化目标如下：
$$
\mathcal{J}_{P P O}(\theta)=\mathbb{E}_{q \sim P(Q), o \sim \pi_{\theta_{\text {old }}}(O \mid q)} \frac{1}{|o|} \sum_{t=1}^{|o|} \min \left[\frac{\pi_{\theta}\left(o_{t} \mid q, o_{<t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{t} \mid q, o_{<t}\right)} A_{t}, \operatorname{clip}\left(\frac{\pi_{\theta}\left(o_{t} \mid q, o_{<t}\right)}{\pi_{\theta_{\text {old }}}\left(o_{t} \mid q, o_{<t}\right)}, 1-\varepsilon, 1+\varepsilon\right) A_{t}\right]
$$

其中，\\( A_t \\)使用GAE（取γ=1）进行计算：
* \\( A_t = \sum_{k=0}^{n-1} \lambda^k \left( r_{t+k} + V(q,o_{<t+k+1}) - V(q,o_{<t+k}) \right) \\)
* 中间token的奖励为0，只在响应结束时计算奖励：
$$
r_{t} =
\begin{cases}
0, & \text{if } t<length(o) \\\\
r(q, o) - \beta \log \left[ \pi_{\theta}^{\text{RL}}(o|q) / \pi^{\text{SFT}}(o|q) \right], & \text{if } t=length(o)
\end{cases}
$$
* \\( V(q,o_{<t}) \\) 从奖励模型初始化，在PPO训练过程与策略网络同步训练。
