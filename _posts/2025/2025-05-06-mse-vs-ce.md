---
title: '为什么说： MSE 隐含了误差服从高斯分布（连续变量），CrossEntroyLoss 则隐含了输出标签符合“多项式分布”（离散变量）？'
date: 2025-05-06
permalink: /posts/2025/05/mse-vs-ce/
tags:
  - MSE
  - CrossEntroyLoss
  - 高斯分布
  - 多项式分布
  - 机器学习
---

均方误差（MSE）与交叉熵损失（CrossEntroyLoss）的本质都是通过**最大似然估计（MLE）**，将分布假设转化为优化目标，从而实现模型训练。这一设计思想体现了统计学习中**分布假设驱动损失函数选择**的核心逻辑。

分析如下：

---

### **1. 均方误差（MSE）损失与高斯分布（连续变量）**
#### **核心逻辑：**
当使用 **MSE 损失函数**时，隐含假设了模型预测值与真实值之间的误差服从 **高斯分布（正态分布）**。这一假设可以通过 **最大似然估计（MLE）** 的数学推导来验证。

#### **数学推导：**
1. **高斯分布的似然函数假设：**
   - 假设每个样本的真实值 `y_i` 与模型预测值 `f(x_i)` 的误差 \\( \epsilon_i = y_i - f(x_i) \\) 服从独立同分布的高斯分布：
     \\[
     \epsilon_i \sim \mathcal{N}(0, \sigma^2)
     \\]
     即：
     \\[
     p(\epsilon_i) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\epsilon_i^2}{2\sigma^2}\right)
     \\]
   - 因此，`y_i` 的条件概率可以表示为：
     \\[
     p(y_i | x_i, f(x_i)) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - f(x_i))^2}{2\sigma^2}\right)
     \\]

2. **最大化对数似然：**
   - 对于 `N` 个独立样本，总似然函数为所有样本概率的乘积：
     \\[
     \mathcal{L}(\theta) = \prod_{i=1}^N p(y_i | x_i, f(x_i;\theta))
     \\]
   - 取对数后得到对数似然：
     \\[
     \log \mathcal{L}(\theta) = -\frac{N}{2} \log(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^N (y_i - f(x_i;\theta))^2
     \\]
   - 最大化对数似然等价于最小化以下目标：
     \\[
     \min_\theta \sum_{i=1}^N (y_i - f(x_i;\theta))^2
     \\]
     这正是 **MSE 损失函数**的形式。

因此，在假设误差服从高斯分布的情形下，MSE Loss 是最大似然估计的直接结果。即：**使用 MSE Loss 隐含了对误差服从高斯分布的假设**。

---

### **2. 交叉熵损失（CrossEntroyLoss）与离散变量的多项式分布**
#### **核心逻辑：**
当使用 **交叉熵损失函数**时，隐含假设了分类任务的标签服从 **多项式分布**（或伯努利分布，当二分类时）。这一假设同样可以通过最大似然估计推导。

#### **数学推导：**
1. **多项式分布的似然函数假设：**
   - 假设分类任务中，每个样本属于 `K` 个类别中的一个，标签 `y_i` 是一个独热编码（One-Hot）向量（例如，\\( y_i = [0, 1, 0]^T \\) 表示第二个类别）。
   - 模型输出 <span>\\( \hat{y}_i = [p_1, p_2, ..., p_K]^T \\)</span> 是预测的概率分布，满足 <span>\\( \sum_{k=1}^K p_k = 1 \\)</span> 且 \\( p_k \geq 0 \\)。
   - 根据 **多项式分布**的定义，标签 `y_i` 的概率为：
     <div>$$ p(y_i | \hat{y}_i) = \prod_{k=1}^K p_k^{y_{ik}} $$</div>
     其中，\\( y_{ik} \\) 是独热编码中第 `k` 类的值（ `0` 或 `1` ）。

2. **最大化对数似然：**
   - 总似然函数为所有样本概率的乘积：
     <div>$$ \mathcal{L}(\theta) = \prod_{i=1}^N \prod_{k=1}^K \hat{y}_{ik}^{y_{ik}} $$</div>
   - 取对数后得到对数似然：
     \\[
     \log \mathcal{L}(\theta) = \sum_{i=1}^N \sum_{k=1}^K y_{ik} \log \hat{y}_{ik}
     \\]
   - 最大化对数似然等价于最小化以下目标：
     \\[
     \min_\theta - \sum_{i=1}^N \sum_{k=1}^K y_{ik} \log \hat{y}_{ik}
     \\]
     这正是 **交叉熵损失函数**的形式。

因此，在假设输出标签符合多项式分布的情形下，CrossEntroyLoss 是最大似然估计的直接结果。即：**使用 CrossEntroyLoss 隐含了对标签服从多项式分布的假设**。

---

### **3. 总结**
- **MSE（高斯分布）：**
  - **适用场景**：回归任务（预测连续值）。
  - **分布假设**：误差服从高斯分布，适合连续变量的建模。
  - **数学特性**：对误差平方敏感，对异常值敏感（因平方放大了误差）。

- **交叉熵（多项式分布）：**
  - **适用场景**：分类任务（预测离散标签）。
  - **分布假设**：标签服从多项式分布，适合离散变量的建模。
  - **数学特性**：直接优化概率分布之间的差异，对异常值相对鲁棒。
