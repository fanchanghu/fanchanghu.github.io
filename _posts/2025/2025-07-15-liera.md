---
title: 'LieRA-用李群理论解决PEFT（参数高效微调）问题'
date: 2025-07-15
permalink: /posts/2025/07/liera/
tags:
  - LieRA
  - PEFT
  - LoRA
  - LLM
  - Lie Group
---

很有意思的一篇论文：[Generalized Tensor-based Parameter-Efficient Fine-Tuning via Lie Group Transformations](https://arxiv.org/abs/2504.00851v2)，作者将待优化参数（W）空间视为流形而非普通的线性空间，并在其上定义Hadamard积为乘法运算，从而得到一个李群（矩阵元素不能为0）。再将对参数的更新（∇W）视为李代数空间中的扰动，通过指数映射拉回李群流形空间进行参数更新。

![Illustration comparing LoRA and LoRA](/images/202507/liera-1.png)

---

## 理论优势分析

这里需要回答2个关键问题：

### 为什么将参数空间视为流形更加合理？

1. **非线性约束 → 流形必然性**
   - 卷积核若要求 **逐元素非零**，则集合
     $$
     \mathcal M=\{W\in\mathbb R^{C_{\mathrm{out}}\!\times C_{\mathrm{in}}\!\times k\times k}\mid W_{i,j,c,l}\neq 0\}
     $$
     对加法不封闭、不含零元，**天然不是线性空间**；
     而 \\((\mathbb R\setminus\{0\})^N\\) 是 **光滑李群**（Hadamard 积群），维度 = \\(C_{\mathrm{out}}C_{\mathrm{in}}k^2\\)。

2. **优化景观观测（optimization landscape）**
   - [Aghajanyan et al. 2020]：大模型微调仅需 **<1% L2 漂移** 即可达 90% 以上性能，表明有效优化轨迹 **落在低维弯曲子流形** 上。
   - 将参数空间显式建模为 **李群**，可让优化器 **沿测地线小步移动**，避免“直线加法”穿出约束面。（如图）

### 为什么在参数微调时，乘法更新优于加法的更新？

1. **保结构**
   - 乘法更新 \\( W\leftarrow W\odot\exp(\Delta) \\) 始终落在 **非零约束面** 内；
     加法 \\( W\leftarrow W+\Delta \\) 可能把权重推到 0 或负值，导致 **ReLU 死亡、梯度消失**。

2. **满秩容量 vs. 低秩瓶颈**
   - LoRA 的 \\(\Delta W=BA\\) 秩 \\(\le r\\)；
     LieRA 的 \\(W\odot\exp(\Delta)\\) 在理论上 **可达满秩**，更灵活捕捉任务特异性。

3. **数值稳定性**
   - 乘法更新是 **比例调整**，对权重尺度不敏感；
     加法更新易出现 **梯度爆炸/消失**。

---

## 工程落地方法

理论上有优势，不代表工程上的可行性。为了能够在工程中应用LieRA，还需要回答下面2个问题：

### LieRA要求参数中不能出现0，但神经网络的参数中，0是经常出现的，如何解决？

| 策略 | 做法 | 额外开销 | 效果 |
|---|---|---|---|
| **ε-clip** | 训练前/每步把 `abs(W)<ε` 的权重设为 `ε` | 一行代码 | 兼容所有含 0 的网络 |
| **稀疏掩码** | 冻结 0 值，仅对非零子集应用 LieRA | 掩码存储 | 极端稀疏场景仍可用 |
| **前置偏移** | 初始化时给全零张量加微小常数 ε | 一次性 | 与 LoRA 切换零成本 |

实验显示（LieRA 原文表 6）：
- ε=1e-8 时，性能与“精确非零版”差距 <0.2%，训练时间/显存几乎不变。

### 将扰动（∇W）从代数空间映射回李群空间，如何解决计算复杂度问题？

1. **逐元素 exp 近似**
   - 由于 ΔW 是**小扰动**（实测 <1% 范数），使用一阶泰勒：
     $$
     \exp(\Delta)\approx I+\Delta
     $$
   - 计算退化到 **一次逐元素乘加**，复杂度与 LoRA 同级。

2. **低秩参数化**
   - 在 **李代数**（线性空间）里仍可用 LoRA/DoRA/PISSA 做低秩分解：
     $$
     \Delta = BA,\quad A\in\mathbb R^{r\times N},\; B\in\mathbb R^{N\times r}
     $$
     最终更新
     $$
     W\leftarrow W\odot(I+BA)
     $$
     训练参数、显存与 LoRA 相同。

     从这里可以看出，LieRA本质上是乘法更新：每次将参数（W）放大或缩小一点；而LoRA是加法更新：每次将参数（W）增加或减小一点点。

---

## LieRA 应用场景

| 参数维度 | 适用场景 | 优势 | 已验证结果 |
|---|---|---|---|
| **高维参数**<br>（4D 卷积核） | ConvNeXt, ResNet, SDXL 卷积层 | 保持空间局部性，满秩更新 | VTAB-1k **↑1.0-1.3pp**；COCO mAP **↑3.8pp** |
| **二维参数**<br>（线性层、Embedding） | LLaMA, DeBERTaV3 的 Q/K/V/Dense 矩阵 | 满秩容量，数值稳定 | GLUE **↑0.8pp**；LLaMA 常识推理 **↑4.3pp** |
| **即插即用** | 与 LoRA/DoRA/PISSA 叠加 | 零代码侵入，参数预算不变 | PISSA+LieRA VTAB **↑1.0pp**，COCO **↑4.2pp** |

> **总结**：无论二维还是高维，LieRA 仅需 **一行 ε-clip + 一行乘法替换**，即可在 **不增加训练成本** 的前提下，带来 **一致且显著** 的性能提升。

## LieRA 实验效果

一句话总结：在CV和NLP任务中都比LoRA好一些，但还达不到全参微调的效果。

详情见原论文。
