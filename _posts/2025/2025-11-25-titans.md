---
title: 'Titans: Learning to Memorize at Test Time 论文学习'
date: 2025-11-25
permalink: /posts/2025/11/titans/
tags:
  - Titans
  - Long-term Memory Module
  - 测试时学习
  - MAC
  - MAG
  - MAL
---

本文是关于 [Titans 架构](https://arxiv.org/abs/2501.00663) 的一个学习总结。论文从线性注意力与循环网络的递推形式出发，提出一种可在推理阶段持续学习的**深度长期记忆模块（LMM）**。此外，作者基于 LMM 提出了三种记忆与注意力集成方式：Memory as a Context（MAC）、Memory as a Gate（MAG）与 Memory as a Layer（MAL），显著增强长程依赖建模能力，与主流线性循环模型相比，兼具效率与表达能力，为下一代大模型架构设计提供了新思路。

## 背景

在 Transformer 架构中，标准注意力机制对长度为 N 的序列需要 `O(N²)` 的显存与计算，当 `N` 较大时成为瓶颈。
线性注意力用核技巧 `φ(·)` 将 Softmax 近似为:

$$
y_i = \frac{\phi(Q_i)^\top \sum_{j=1}^i \phi(K_j)V_j}{\phi(Q_i)^\top \sum_{j=1}^i \phi(K_j)} \tag{1}
$$

当选择核函数为单位矩阵时，注意到分母是关于 token 位置 `i` 的标量值，可以吸收到查询 `Q_i` 中，上述公式可以写成如下递归形式：

$$
M_t = \sum_{j=0}^t V_j K_j^\top = M_{t-1} + V_t K_t^\top, \quad y_t = M_t Q_t \tag{2}
$$

另一方面，传统 RNN 的隐藏状态 \\( h_t = f(h_{t-1}, x_t) \\) 也是类似的递归模式。其中，隐藏状态 `h_t` 被视为一个记忆单元，并且循环过程可以被分解为记忆单元中的读取和写入操作。

从这个角度来看，线性Transformer等价于将键 `K` 和值 `V` 以加法方式压缩并写入一个矩阵值存储单元 `M`。`M` 可以被视作“记忆”，每一步前向更新把新键值对 `K, V` “写”入，并通过查询 `Q` “读”取。

基于以上信息，论文提出一个更一般的 **测试时记忆（Test-Time Memory）** 概念，它指出记忆应当：
1. 在推理阶段仍可被写入/擦除；
2. 同时维护「短时精确依赖」与「长时抽象关联」；
3. 容量与结构可随任务自适应，而非固定矩阵。

## 测试时记忆

与传统「Agent 记忆」不同（对话历史缓存、RAG 等，通常在模型外部检索），**测试时记忆**指参数本身在推理期间继续更新，从而把「上下文」编码进权重。论文将其拆成两类：

| 类型 | 是否上下文相关 | 参数是否可变 | 作用 |
|---|---|---|---|
| 长期记忆（Long-term Memory, LMM） | ✅ 依赖输入 | ✅ 推理期仍更新 | 把远程序列抽象存入深度网络权重 |
| 持久记忆（Persistent Memory） | ❌ 任务相关、输入无关 | ❌ 训练后固定 | 存储跨样本的通用知识，类似 FFN 的 Key-Value |

### 长期记忆（Long-term Memory）

#### 1. Surprise Metric
人脑对「违反预期」的事件记得更牢。所以可以用输入与当前记忆的差异衡量 surprise：

$$
\begin{align}
& M_t = M_{t-1} - \theta_t \underbrace{\color{green} \nabla_{M_{t-1}}\, \ell(M_{t-1}; x_t)}_{suprise} \tag{3} \\
& \ell(M; x_t) = \|M(k_t) - v_t\|_2^2 \tag{4} \\
& k_t = W_K x_t ,\; v_t = W_V x_t
\end{align}
$$

其中：
* `k_t, v_t` 与 Transformer 中的键和值的计算方式相同。
* `M` 表示对历史 token 的记忆。
* `ℓ(.)` 衡量了记忆 `M` 对 `x` 的预测偏差，偏差越大，模型越**惊讶（suprise）**。
* `∇ℓ(.)` 是惊讶对记忆参数的梯度，表示 `x_t` 对记忆 `M` 的更新程度。由于 `ℓ(.)` 具有 MSE 形式，模型越惊讶 → 梯度（的模）越大 → `x_t` 被记住的越多。

注意，上述记忆模块是一个 LLM 中的子模型，即：大模型在推理时，基于损失函数 `ℓ(.)` 在内部学习了一个小模型（M），来记忆过去的信息。我们可以将该架构分为外循环和内循环两个部分：

* **内循环**优化记忆 `M`，其参数在测试时仍然可以更新；
* **外循环**优化 LLM 的其他参数，包括内循环的超参数，在训练结束后锁定。
* `W_K, W_V, θ_t` 等属于记忆模块的超参数，由外循环在训练时确定。

#### 2. 带动量的记忆更新
心理学上，一个足够令人惊讶事件（token），往往可以在很长的时间范围内引起我们的注意。为此，作者引入动量 `S_t`：

$$
\begin{align}
M_t &= (1-\alpha_t)\, M_{t-1} + S_t \tag{5} \\
S_t &= \eta_t\, S_{t-1} - \theta_t\, \nabla\ell(M_{t-1}; x_t) \tag{6}
\end{align}
$$

公式（5）、（6）整体等价于**动量 + 权重衰减**。`α_t` 相当于「遗忘门」。

`α_t, θ_t, η_t` 可设计为数据依赖的超参数，由外循环生成。

#### 3. 深度记忆架构
从前面的分析可以看出，线性注意力相当于使用矩阵值 `M = 𝑊 ∈ R^(𝑑×𝑑)` 作为记忆。其优化目标为 \\( \ell(M; x_t) = \|Wk_t - v_t\|_2^2 \\) 。

这是一个线性回归目标，且其存在解析解，因此无需反向传播。但它假设历史数据的潜在依赖关系是线性的，可能面临表现力不足的问题。

作者提出使用多层的 MLP 或其他神经网络结构作为记忆模型，期望解决线性模型表现力不足的问题，并取得了更好的结果。

#### 4. 记忆检索
对于给定输入 token `x_t`，将其投影为查询条件 `q_t` 后，直接前向传播即可读出记忆内容：
$$
y_t = M^*(q_t), \quad q_t = W_Q x_t \tag{7}
$$

### 持久记忆（Persistent Memory）

用一组与输入无关的可学习向量 `[p_1, ..., p_n]` 拼接在输入序列开头，充当「任务先验」。从注意力视角看，它们相当于额外的 Key-Value 对，可缓解 causal mask 对早期 token 的偏重，为模型提供全局可检索的知识槽。

### 长期记忆的并行训练

由于公式（5）、（6）是时序更新的，不利于并行训练，作者提出了一种可并行训练的改进版本。

把序列按大小 `C` 切块（chunk），每个 chunk 内记忆 `M` 参数冻结，只在 chunk 的末尾进行一次小批量梯度下降以更新记忆参数，作为下一个 chunk 的记忆使用。从而提升记忆更新效率。如下：

$$
\begin{align}
M_t &= (1-\alpha_t)\, M_{t-1} - \theta_t \nabla \ell(M_{t-1};x_t) \\
    &= \beta_t M_{t'} - \sum_{i=t'+1}^t \Bigl( \theta_i \frac{\beta_t}{\beta_i} \nabla \ell(M_{t'};x_i) \Bigr) , \quad \text{其中} \beta_i = \prod_{j=1}^i (1-\alpha_j) \tag{8} \\
    &= (1-\alpha_t)^C M_{t'} - \theta_t \sum_{i=t'+1}^t \Bigl( (1-\alpha_t)^{t-i} \nabla \ell(M_{t'};x_i) \Bigr) , \quad \text{当 α, θ 在 chunk 内固定。} \tag{9} \\
\end{align}
$$

以上梯度项也可以替换为公式（6）的动量项，并且，当 `α, θ, 𝜂` 在 chunk 内固定时，记忆的更新允许在 chunk 内并行训练（使用公式 9）。

## 记忆模块与 LLM 架构集成

**问题**：有了可写的长期记忆后，如何将其集成到现有的 LLM 架构中？论文提出三种方式，对应三种 Titans 变体。

### 记忆即上下文（Memory as a Context, MAC）

![MAC](/images/202511/titans-1.png)
*图1： MAC 架构图*

我们约定以下符号：
* `x`: 给定的、长度为 `N` 的 token 序列，`x ∈ R^(N, d)`；
* `s^(𝑖)`: 将 `x` 分成大小为 `C` 的片段 `s^(𝑖)`，其中 `i ∈ [1, N/C]`，`s^(𝑖) ∈ R^(C, d)`；
* `t`: 当前的片段编号，因此 `s^(𝑡)` 为当前上下文；
* `M_{𝑡−1}`: 片段 `s^(𝑡)` 之前的长期记忆状态；
* `q^(t)`: 当前片段 `s^(𝑡)` 对应的注意力查询： `q^(t) = W_Q s^(𝑡)`， `q^(t) ∈ R^(C, d)`；
* `p^(n)`: 长度固定为 `n` 的、上下文无关的、可学习的持久记忆（Persistent Memory）token 序列。它总是附加在当前上下文序列的最前面。 `p^(n) ∈ R^(n, d)`

在MAC架构中，如图所示：
1. 中间的 Core 层是 LLM 的核心推理流程，展开的部分为注意力计算过程。除了额外附加的持久记忆 token 和长期记忆 token，该部分与传统的基于 Transformer 解码器的 LLM 相似。
2. 查询（Retrieval）： \\( h^{(t)} = M_{t-1}(q^{(t)}) \\)， \\( h^{(t)} \in R^{(C, d)} \\)
3. 拼接上下文： \\( \overline{s}^{(t)} = p^{(i)} \| h^{(t)} \| s^{(t)} \\)， \\( \overline{s}^{(t)} \in R^{(n+2C, d)} \\)
4. 计算注意力： \\( y^{(t)} = Attn(\overline{s}^{(t)}, s^{(t)}) \\)， \\( y^{(t)} \in R^{(C, d)} \\)
5. 基于公式（4）定义的损失函数，进行反向传播，保存每个 token 的梯度。
6. 根据公式（6）计算记忆动量 `S^(t)`，使用公式（9）更新记忆。
7. 用记忆更新当前输出： \\( \overline{y}^{(t)} = y^{(t)} ⊗ M_t(y^{(t)}) \\) ，其中，⊗的含义作者未直接说明，推测为张量拼接操作。拼接后为形状 `(C, 2d)` 的张量再送入后续 MLP 中。

![MAC-Attn](/images/202511/titans-2.png)
*图2： MAC 的注意力*

从注意力视角看，MAC 架构下，token 之间的关系如图2：
1. 左图，片段内 token 之间通过因果注意力关联，片段间 token 无关联。
2. 中图，绿色的 token 代表之前所有片段的长期记忆，片段内 token 之间通过因果注意力关联，后续片段的 token 可以通过长期记忆间接感知之前片段的 token。
3. 右图，红色为固定的持久记忆 token，它包含训练中学习的、上下文无关的知识。

### 记忆即门控（Memory as a Gate, MAG）

![MAG](/images/202511/titans-3.png)
*图3： MAG 架构图*

与 MAC 的主要区别：
1. 不对输入序列进行分段，而是使用带有前缀的滑窗注意力机制。
2. 对长期记忆的使用方式不同：MAC 将长期记忆视为潜在 token，而 MAG 将其视为门控单元。
3. 对记忆的更新时机：MAC 在片段结尾更新；MAG 在 token 移除窗口时更新。这可能意味着 MAG 需要更频繁的记忆更新。（论文未直接提及，此为推测）

![MAC-Attn](/images/202511/titans-4.png)
*图4： MAG 的注意力*

从注意力视角看，MAG 架构下，token 之间的关系如图4：
1. 左图，窗口内 token 之间通过因果注意力关联，窗口外的 token 无关联。
2. 中图，绿色的 token 被移除窗口后更新长期记忆，它们透过长期记忆影响机制，以门控的方式影响输出。受遗忘因子影响，越早期的 token 影响越弱。
3. 右图，红色为固定的持久记忆 token，它们总是附加在窗口 token 前，包含了训练中学习的、上下文无关的知识。

### 记忆即层（Memory as a Layer, MAL）

![MAL](/images/202511/titans-5.png)
*图5： MAL 架构图*

这种架构设计在文献中更为常见，它将注意力（完整或滑窗）与记忆模型堆叠在一起。
作者认为，这种设计的最大缺点是模型的能力受到每一层的限制，因此它无法利用注意力和神经记忆模块的互补数据处理能力。
后续的实验也证明，它比 MAC 和 MAG 的能力略差一些。

## 小结

1. 提出了一种新的 LLM 测试时学习的方法。
2. 提出了一种记忆模块的并行训练方法。
3. 提出了记忆模块与 Transformer 架构集成的三种方法。

作者的实验表明，在 2M token 上下文、needle-in-haystack、BABILong 推理、时间序列与 DNA 建模等任务上，Titans 均优于同等规模的 Transformer 与最新线性循环模型，且训练吞吐量保持线性增长。
