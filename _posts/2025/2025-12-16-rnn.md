---
title: '循环神经网络及其变体：理解 RNN, LSTM, GRU'
date: 2025-12-16
permalink: /posts/2025/12/rnn/
tags:
  - 循环神经网络
  - RNN
  - LSTM
  - GRU
---
传统机器学习中，处理序列数据（如文本、语音、时间序列）经常需要使用的技术是 RNN。但普通的 RNN 的结构缺陷导致模型在处理长序列时极易出现梯度消失。为此，LSTM 通过引入“细胞状态”开辟了梯度传播的高速公路，而 GRU 则在此基础上进一步简化了参数规模。
本文将从技术底层介绍这三种模型，主要包括模型结构、长距离依赖的解决、梯度消失/爆炸的原因及解决等。

## 引言 (Introduction)

### 什么是序列数据？

序列数据是指数据点之间存在内在的 **时间或逻辑顺序关联** 的数据。

* **特点：** 前后的数据单元相互依赖、相互影响。
* **示例：** 文本（词语顺序）、语音（声波序列）、时间序列（股票价格、传感器读数）。
* **挑战：** 传统的深度学习模型（如 FFN 和 CNN）难以有效捕捉这种 **时间依赖性** 和 **长距离依赖性**。

## 循环神经网络 (Recurrent Neural Network, RNN)

### RNN 的核心思想

![RNN](/images/202512/rnn-1.png)
*图1： RNN 架构示意*

如图1，RNN 是专门为处理序列数据而设计的神经网络。它的核心思想是：**信息在时间步之间循环传递。**

* **循环连接：** 网络在处理序列的每一步时，都会将上一步的隐藏状态（记忆）作为输入的一部分。
* **权值共享：** RNN 在整个序列中使用的 **权重矩阵 `W` 是共享的**，这大大减少了模型的参数数量。

### RNN 的计算过程

在时间步 `t`，隐藏状态 `h_t` 和输出 `y_t` 的计算如下：

* **隐藏状态更新：**
    $$
    h_t = \text{tanh}(W_{hh}h_{t-1} + W_{xh}x_t + b_h) \tag{1}
    $$
* **输出计算：**
    $$
    y_t = W_{hy}h_t + b_y \tag{2}
    $$

### RNN 的局限性：梯度消失问题

RNN 在处理较长的序列时，会遇到两个主要问题：

#### **长距离依赖困难 (Long-Term Dependencies)**
早期时间步（序列开头）的信息经过多次循环后会被稀释，难以影响到后期时间步的计算。原因在于，`tanh` 激活函数会将所有的值压缩到 `(-1, 1)` 之间，随着时间步的累积，早期时间步（序列开头）的信息将经过多次压缩，迅速接近于 `0`。从模型表现来看，就是遗忘了早期的信息。

#### **梯度消失与爆炸 (Vanishing/Exploding Gradients)**
在使用 **基于时间的反向传播 (BPTT)** 算法训练时，梯度会呈指数级衰减或增长，导致梯度消失和梯度爆炸。问题的核心在于 RNN 的前向是一个递归过程，而这种递归在使用链式法则求导时，会成为一个关于时间步长的 **连乘** 过程。

我们考虑最终时刻 `t` 的损失函数 `L_t` 对某一时刻 `k` 的隐藏状态 `h_k` 的梯度。根据链式法则，要计算 `L_t` 对 `h_k` 的梯度，需要沿着时间链反向传播：

$$
\frac{\partial L_t}{\partial h_k} = \frac{\partial L_t}{\partial h_t} \frac{\partial h_t}{\partial h_{t-1}} \frac{\partial h_{t-1}}{\partial h_{t-2}} \cdots \frac{\partial h_{k+1}}{\partial h_k} \tag{3}
$$

RNN 隐藏状态的计算公式是：

$$
h_{j+1} = \text{tanh}(W h_j + U x_{j+1} + b)  \tag{4}
$$

因此：

$$
\frac{\partial h_{j+1}}{\partial h_j} = \operatorname{diag}(\tanh'(...)) W \tag{5}
$$

代入公式（3），并取范数，可得：

$$
\begin{align}
\| \frac{\partial L_t}{\partial h_k} \| &= \| \frac{\partial L_t}{\partial h_t} \| \cdot \| \operatorname{diag}(\tanh'(...)) \|^{t-k-1} \|W\|^{t-k-1} \\
&= \| \frac{\partial L_t}{\partial h_t} \| \cdot C \cdot \|W\|^{t-k-1} \tag{6} \\
\end{align}
$$

其中 `||W||` 为谱范数 (Spectral Norm)。

显然 `C<1`，如果 `||W|| < 1`， 公式（6）将迅速趋于 `0`，导致梯度消失；如果 `||W|| > 1`，在 `tanh` 的近线性区域（`0` 值附近），公式（6）将迅速趋于 `∞`，导致梯度爆炸。

## 长短期记忆网络 (Long Short-Term Memory, LSTM)

LSTM 被设计来解决 RNN 的梯度消失问题，使得模型能够有效捕捉和存储长距离依赖信息。

### LSTM 的核心创新：细胞状态与门机制

![LSTM](/images/202512/rnn-2.png)
*图2： LSTM 架构示意*

如图2所述，LSTM 引入了两个核心组件：
1.  **细胞状态 (Cell State)：** LSTM 将 RNN 的隐藏状态 `h_t` 拆分为 `C_t` 和 `h_t` 两个部分，前者专职记忆信息，后者专职输出信息。
    * `C_t` 是 **内部长线记忆**
      * 它是 **私有** 的，只在时间轴上水平流动，不直接对外输出信息。
      * 它采用了 **加法更新**，就像一条干净的传送带，专门负责解决梯度消失。
    * `h_t` 是 **外部短期表现**
      * 它是 **公开** 的，既水平传给下一时刻，也垂直传给上层。
      * 它是从 `C_t` 中经过“输出门”过滤、提取出来的 **精简版** 记忆。
2.  **门机制 (Gating Mechanism)：** 用于控制信息的流入、流出和遗忘。一般由 `[h_{t-1}, x_t]` 投影后经过 Sigmoid 激活函数得到。

### LSTM 的三个门详解

LSTM 单元包含三个关键的门：

#### 1. 遗忘门 (Forget Gate, `f_t`)

决定要从上一个细胞状态 `C_{t-1}` 中 **丢弃** 多少信息。

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \tag{7}
$$

#### 2. 输入门 (Input Gate, `i_t`) 与候选状态 \\( \tilde{C}_t \\)

决定有多少 **新的信息** 需要添加到当前细胞状态中。

$$
\begin{align}
i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \tag{8} \\
\tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) \tag{9}
\end{align}
$$

#### 3. 细胞状态更新 (Cell State Update)

这是 LSTM 避免梯度消失的关键：旧信息被遗忘门 `f_t` 筛选后，与被输入门 `i_t` 筛选的新信息相加。

$$
C_t = f_t * C_{t-1} + i_t * \tilde{C}_t \tag{10}
$$

#### 4. 输出门 (Output Gate, `o_t`) 与隐藏状态更新

决定基于当前的细胞状态 $C_t$，输出多少信息到隐藏状态 $h_t$。

$$
\begin{align}
o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \tag{11} \\
h_t &= o_t * \tanh(C_t) \tag{12}
\end{align}
$$

### 为什么 LSTM 能够解决梯度消失和梯度爆炸
在 LSTM 的细胞状态更新中：

$$
C_t = f_t * C_{t-1} + i_t * \tilde{C}_t \tag{13}
$$

我们对前一时刻的细胞状态 `C_{t-1}` 求梯度：

$$
\frac{\partial C_t}{\partial C_{t-1}} = \mathbf{f_t} + \frac{\partial f_t}{\partial C_{t-1}} C_{t-1} + \frac{\partial (i_t * \tilde{C}_t)}{\partial C_{t-1}} \tag{14}
$$

这里关键在于将信息的更新从 RNN 的 “乘法挤压” 变为了 “门控加法”。`f_t` 项不经过任何非线性函数的收缩：
* 如果模型发现当前的远距离信息对损失函数很重要，反向传播的压力会迫使 `f_t` 的值趋向于 `1`。此时梯度几乎以常数级 `1` 回传，形成了时序上的“记忆高速公路”。
* 公式（13）与残差网络的跳层连接有异曲同工之妙，残差网络的梯度（\\( 1 + \frac{\partial \text{Residual}}{\partial x} \\)）中的 `1`，起到了“开门”的作用，确保梯度能够在更深的网络中回传。从这个意义上说，**LSTM 相当于在时序维度上实现了一个门控版的残差连接。**——它不仅能让梯度传得远，还能通过学习 `f_t` 灵活决定“哪些梯度该传，哪些该断”。
* 在工程上，我们也可以将 `f_t` 默认初始化为较大的值（通过设置遗忘门的偏置），强制为梯度流“开门”，让模型学会长距离依赖，再慢慢学习如何遗忘。

## 门控循环单元 (Gated Recurrent Unit, GRU)

LSTM 虽然解决了梯度消失和梯度爆炸的问题，但它参数量较多，实现复杂，因此，后来又提出GRU。
GRU 是 LSTM 的一种变体，旨在保持处理长距离依赖的能力，同时减少参数数量，提高计算效率。

### GRU 的简化

![GRU](/images/202512/rnn-3.png)
*图3： GRU 架构示意*

1.  **两个门：** 将 LSTM 的三个门简化为 **更新门** 和 **重置门**。
2.  **合并状态：** 将 LSTM 的两个状态（`h_t` 和 `C_t`）**重新合并为一个隐藏状态 `h_t`**。

### GRU 的两个门

#### 1. 更新门 (Update Gate, `z_t`)

结合了 LSTM 的遗忘门和输入门的功能，控制着保留多少 **旧信息 \\(h_{t-1}\\)** 和接受多少 **新信息 \\(\tilde{h}_t\\)**。

$$z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z) \tag{15}$$

#### 2. 重置门 (Reset Gate, `r_t`)

决定如何将 **历史信息 \\(h_{t-1}\\)** 与当前输入 `x_t` 结合来计算新的候选状态。如果 `r_t` 接近 `0`，则忽略历史信息。

$$r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r) \tag{16}$$

### 4.3 GRU 的状态更新

* **新的候选隐藏状态 \\(\tilde{h}_t\\)**：
    $$\tilde{h}_t = \tanh(W \cdot [r_t * h_{t-1}, x_t] + b) \tag{17}$$
    > $r_t * h_{t-1}$ 表示对前一状态进行重置的程度。
* **最终隐藏状态 \\(h_t\\)**：使用更新门 `z_t` 来进行线性插值。
    $$h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t \tag{18}$$
    > `1 - z_t` 控制保留旧信息，`z_t` 控制加入新信息。它意味着 GRU 强制要求模型在“完全保留旧信息”和“完全采用新信息”之间做平衡，这比 LSTM 独立控制遗忘和输入要更加稳定，也是 GRU 参数更少、更容易收敛的原因之一。

## 总结与对比

| 特性 | RNN (Plain) | LSTM | GRU |
| :--- | :--- | :--- | :--- |
| **解决长依赖** | 差 | 优 | 优 |
| **核心状态** | 隐藏状态 `h_t` | 隐藏 `h_t` + 细胞 `C_t` | 隐藏状态 `h_t` (合并) |
| **门数量** | 0 | 3 (遗忘、输入、输出) | 2 (更新、重置) |
| **参数数量** | 最少 | 最多 | 较少 |
| **训练速度** | 最快 | 较慢 | 较快 |
| **复杂性** | 低 | 高 | 中等 |
