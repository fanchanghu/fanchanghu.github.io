---
title: 'Nested Learning: The Illusion of Deep Learning Architectures 论文学习'
date: 2025-12-03
permalink: /posts/2025/12/nl/
tags:
  - Nested Learning
  - 嵌套学习
  - 持续学习
  - Titans
  - LLM
---

本文是对论文 [Nested Learning: The Illusion of Deep Learning Architectures](https://neurips.cc/virtual/2025/loc/san-diego/poster/116123) 的学习总结。传统深度学习将模型的训练（Learning）与推理（Inference）视为两个严格分离的阶段。训练阶段参数更新（学习），推理阶段参数固定（应用）。
论文打破了这一传统界限，提出了一个嵌套学习（Nested Learning, NL）范式。它将模型的参数更新、优化器状态（如动量）、甚至是注意力机制中的上下文存储，全部视为一种 **联想记忆（Associative Memory）** 系统。通过引入了 **更新频率（Update Frequency）** 的概念，不同的优化问题可以以不同的频率运行，从而为模型的不同部分分配不同的“时间尺度”来处理信息。
该架构为设计下一代更具表现力、能够持续学习和自我改进的模型架构指明了新的方向。

## 联想记忆 (Associative Memory)

### 定义
给定一组由键 `K` 和值 `V` 组成的数据 `(K, V)`，联想记忆被定义为一个算子： `M: K -> V`，旨在建立键与值之间的映射。

为了从数据中学习这种映射，我们需要定义一个目标函数 `L(⋅)` 并求解以下优化问题：

$$
M^* = \arg \min_{M} \mathcal{L}(M(K); V) \tag{1}
$$

### 关键要素
* **键（Keys）**
* **值（Values）**
* **记忆 (Memory)**：即算子 `M` 本身。
* **目标函数**：衡量键 `K` 从记忆 `M` 中召回的值 `M(K)` 与实际值 `V` 差异的函数，比如均方误差（MSE）等。

下面我们看几个具体实例：

### 单层 MLP 的梯度下降是联想记忆
以单层、无激活函数的前馈网络（即线性层）训练为例，使用梯度下降更新其参数 `W` 可以被重新表述为一个优化问题。

$$
\begin{align}
W_{t+1} &= W_t - \eta_{t+1} \nabla_{W_t} \mathcal{L}^{model}(W_t;x_{t+1}) \\
        &= W_t - \eta_{t+1} x_{t+1} \left[ \nabla_{y_t} \mathcal{L}^{model}(W_t;x_{t+1}) \right]^\top \\
        &= W_t - \eta_{t+1} x_{t+1} u_{t+1}^\top , \quad \text{其中 } u_{t+1} = \nabla_{y_t} \mathcal{L}^{model}(W_t;x_{t+1}) \tag{2}
\end{align}
$$

其中 `(x, y)` 分别对应 网络输入和输出。

\\( u_{t+1} \\) 被解释为输出空间的局部惊讶信号（Local Surprise Signal, LSS），因为它衡量了记忆系统对 `x_{t+1}` 的预测能力。

公式（2）等价于求解以下优化问题：

$$
W_{t+1} = \arg \min_{W} \left[ \langle W^\top x_{t+1} , u_{t+1} \rangle + \frac{1}{2\eta_{t+1}}\| W - W_t \|^2  \right] \tag{3}
$$

我们对照联想记忆的关键要素，可以发现：
* **键（Keys）**： \\( x_{t+1} \\)
* **值（Values）**：\\( u_{t+1} \\)，即： \\( \nabla_{y_t}\mathcal{L}^{model}(W_t;x_{t+1}) \\)
* **记忆 (Memory)**：\\( W_t \\) 是更新前的记忆，\\( W_{t+1} \\) 是更新后的记忆
* **目标函数**： 见公式（3）。如果将第二项视为正则项，那么该目标等价于最小化 \\( W^\top x_{t+1} \\) 与 \\( u_{t+1} \\) 的内积

`W` 实际学习了映射： \\( W: x_{t+1} \to -u_{t+1} \\) 。因此，由参数 `W` 定义的线性层可以被视为一个联想记忆系统。

### 动量是联想记忆

当引入动量（Momentum）机制来改进梯度下降时，更新规则变为：

$$
\begin{align}
W_{t+1} &= W_t + m_{t+1} \\
m_{t+1} &= m_t - \eta_{t+1} \nabla_{W_t} \mathcal{L}^{model}(W_t; x_{t+1}) \\
        &= m_t - \eta_{t+1} x_{t+1} u_{t+1}^\top \tag{4}
\end{align}
$$

上述公式（4）实际上是在求解如下优化问题：

$$
m_{t+1} = \arg \min_{m} \left[ \langle m^\top x_{t+1}, u_{t+1} \rangle + \frac{1}{2\eta_{t+1}} \|m - m_t\|^2 \right] \tag{5}
$$

对照联想记忆的要素：
* **键（Key）**：\\( x_{t+1} \\)
* **值（Values）**：\\( -u_{t+1} \\)，即： \\( -\nabla_{y_t}\mathcal{L}^{model}(W_t;x_{t+1}) \\)
* **记忆（Memory）**：动量矩阵 `m`
* **目标函数**：见公式（5）

动量 `m` 实际学习了如下映射： \\( m: x_{t+1} \to -u_{t+1} \\) 。因此，由参数 `m` 定义的动量可以被视为一个联想记忆系统。使用带动量的梯度下降训练的线性层，则成为一个**2层嵌套的联想记忆系统**。

### 线性注意力是联想记忆

线性注意力（Linear Attention）机制通常具有如下递归形式：

$$
\begin{align}
\mathcal{M}_t &= \mathcal{M}_{t-1} + v_t k_t^\top \tag{6} \\
y_t &= \mathcal{M}_t q_t \tag{7}
\end{align}
$$

公式（6）实际等价于优化以下目标：

$$
\mathcal{M}_t = \arg \min_{\mathcal{M}} \left[ -\langle \mathcal{M} k_t, v_t \rangle + \frac{1}{2} \|\mathcal{M} - \mathcal{M}_{t-1}\|^2 \right] \tag{8}
$$

对照联想记忆的要素：
* **键（Key）**：输入序列的键向量 `k_t`
* **值（Value）**：输入序列的值向量 `v_t`
* **记忆（Memory）**：注意力状态矩阵 `M`
* **目标函数**：见公式（8）

记忆 `M` 实际在学习如下映射： \\(M: k_t \to v_t\\)。说明线性注意力实际上是一个**在线学习的联想记忆**。

可以把 `M` 类比为一个 `map` 集合对象：给定 `k`，返回 `v` 。说明 `M` （以压缩方式）存储了上下文 `(k, v)` 对之间的关联。不同的是， `map` 返回唯一确定的值，可以视为对所有的值的确定性加权和（即其中一个值的权重为 `1`，其他值的权重为 `0`）。而 `M` 返回的是非确定性加权和。从这个意义上看， `map` 是 `M` 的特例。

因此，Transformer 的推理过程本身就是一种嵌套优化：外层是训练好的投影矩阵（Q,K,V），内层是实时优化的注意力状态。

---

## 改进联想记忆

上述三种联想记忆（标准梯度下降、动量、线性注意力）隐含的优化目标函数都具有内积的形式，其内部运作机制实际上都是线性的。可以从以下两个方面来改进联想记忆系统，从而获得更强大的优化器或模型架构：

1.  **引入非线性**：增强记忆模块的表达能力。
2.  **使用其他形式的目标函数**：例如均方误差（MSE），以更好地处理数据间的相关性。

论文给出了 2 个具体实例：

### 在动量中引入非线性

#### 动量是无值联想记忆
我们重新审视公式（4）的动量更新机制，它也等价于求解以下优化目标（忽略正则项）：

$$
\min_{m} \langle m\nabla_W\mathcal{L}^{model}(W_{i};x_{i})^{\top}, I \rangle \tag{9}
$$

对照联想记忆的定义：
* **键 (Key)**：梯度 \\( \nabla_W \mathcal{L}^{model} \\)
* **值 (Value)**：单位矩阵 `I`
* **记忆（Memory）**：动量 `m`
* **目标函数**：见公式（9）

由于目标“值”仅仅是一个恒定的、不包含任何特定信息的单位矩阵，标准动量被称为**无值（Value-less）联想记忆**。它仅仅是将梯度（线性的）压缩进参数中，而没有建立梯度与特定目标值之间的映射。相比之下，Preconditioned SGD（如 Shampoo）则通过引入特定的 `P_i`（如海森矩阵）作为值，使其变为有值联想记忆。

#### 将动量参数化为神经网络
我们将动量 `m` 从一个简单的矩阵扩展为一个具有参数的神经网络（如多层 MLP）。这意味着动量不再只是存储值，而是通过一个函数 `m(⋅)` 来动态预测更新方向：

$$
\begin{align}
g_{t+1} &= \nabla_W \mathcal{L}^{model}(W_{i};x_{i})^{\top} \\
W_{t+1} &= W_t + m_{t+1}(g_{t+1}) \tag{10} \\
m_{t+1} &= \alpha_{t+1} m_t - \eta_{t+1} \nabla \mathcal{L}^{(2)}(m_t; g_{t+1}) \tag{11}
\end{align}
$$

这里，`m(⋅)` 是一个非线性的深度网络。我们不仅更新主参数 `W`，还在每一步通过内部目标 `L^(2)` 更新动量网络的参数。

> * Muon 优化器是一种非线性动量优化器
> 论文指出，如果 `m` 取 Newton-Schulz 迭代（一种矩阵求逆的近似算法），内部目标仍然使用公式（9）的内积形式，公式（10）、（11）就等价于 **Muon 优化器** 。这揭示了 Muon 的本质是在动量记忆的输出端增加了特定的非线性处理。

### 在梯度下降中引入均方误差

如公式（3）所示，标准的梯度下降更新隐含了最小化“参数与梯度”的内积。但是，基于内积形式的优化目标只是历史数据的简单累加（如公式 12），无法捕捉样本间的依赖关系。

$$
\begin{align}
W_{t+1} &= W_t - \eta_{t+1} x_{t+1} u_{t+1}^\top \\
W_{t+1} &= W_0 - \sum_{i=1}^{t+1} \eta_t x_t u_t^\top \tag{12}
\end{align}
$$

为了解决这个问题，论文提出把目标函数改为如下均方误差（MSE）形式，仿照之前的做法，添加正则化项后，得到：

$$
\min_{W}[\| W^\top x_{t+1} + u_{t+1} \|^2 + \frac{1}{\eta_{t+1}} \| W - W_t \|^2] \tag{13}
$$

上述目标存在解析解，对其求导并令导数为0，可得：

$$
\begin{align}
& 2 \eta_{t+1} x_{t+1} (W^\top x_{t+1} + u_{t+1})^\top + 2(W - W_t) = 0 \\
& \eta_{t+1} x_{t+1} x_{t+1}^\top W + \eta_{t+1} x_{t+1} u_{t+1}^\top + W - W_t = 0 \\
& (I + \eta_{t+1} x_{t+1} x_{t+1}^\top) W = W_t - \eta_{t+1} x_{t+1} u_{t+1}^\top \\
& W = (I + \eta_{t+1} x_{t+1} x_{t+1}^\top)^{-1}(W_t - \eta_{t+1} x_{t+1} u_{t+1}^\top) \tag{14} \\
\end{align}
$$

由于 \\( (I + \eta_{t+1} x_{t+1} x_{t+1}^\top)^{-1} = I - \frac{\eta_{t+1}}{1+\eta_{t+1} x_{t+1}^\top x_{t+1} }x_{t+1} x_{t+1}^\top \\) ，不妨令 \\( \beta_{t+1} = \frac{\eta_{t+1}}{1+\eta_{t+1} x_{t+1}^\top x_{t+1} } \\) ，则：

$$
\begin{align}
& W = (I - \beta_{t+1} x_{t+1} x_{t+1}^\top)(W_t - \eta_{t+1} x_{t+1} u_{t+1}^\top) \\
& W = (I - \beta_{t+1} x_{t+1} x_{t+1}^\top)W_t - \beta_{t+1} x_{t+1} u_{t+1}^\top \tag{15}
\end{align}
$$

由于学习率 `η` 通常很小， 此时 `η ≈ β`。**上述公式（15）与论文中的公式（29）核心差异在于遗忘项中的系数差异，考虑单步梯度下降时不应该过多的遗忘之前的信息，公式（15）应该是更合理的结论。**

> 论文中从公式（27）到公式（29）缺少一些必要信息，比如公式（27）中的正则化项。我尝试为其添加正则化项，得到公式（13），并推导出公式（15）。
> 尽管仍然与原论文的结论存在差异，但在原论文中，这块并没有看到详细的推导过程 ...

与标准 SGD 相比，公式（15）中 `W_t` 的系数多了 \\( - \beta_{t+1} x_{t+1} x_{t+1}^\top \\) 这一项。这一项起到了 **遗忘** 的作用。它从当前权重 `W_t` 中移除了与当前输入 `x_{t+1}` 平行的分量，防止新信息的写入干扰到已存储的正交信息。这使得模型在进行“上下文学习”时能更有效地利用容量，这也是 HOPE 架构中内部优化器的核心原理。

---

## 嵌套优化 (Nested Optimization)

既然模型的各个组件（参数、优化器状态、注意力状态）本质上都在运行某种优化过程，论文提出了 **嵌套学习（Nested Learning, NL）** 范式，利用 **频率** 来组织这些过程。

### 频率的定义
对于模型中的任意组件 \\( A \\)（参数或非参数模块），其**更新频率 \\( f_A \\)** 定义为：**在单位“时间”内，该组件更新的次数**。

* **单位“时间”**：通常定义为模型中最慢的部分的更新周期，也就是说，模型中最慢的部分，其频率为 `1`。注意，这种定义下，频率是模型内的相对概念，跨模型比较频率没有意义。
* **层级（Levels）**：根据频率大小，我们可以将模型分解为有序的层级。频率越高，位于越底层（处理快速变化的信息）；频率越低，位于越高层（处理长期稳定的信息）。

### 从频率视角看机器学习
* 传统机器学习是嵌套学习的一个特例：
    * **训练阶段**：所有参数频率为 1（每步都更新）。
    * **推理阶段**：所有参数频率为 0（完全冻结）。
* 嵌套学习的核心创新：
    * 打破二元对立，允许频率在 \\( 0 \sim \infty \\) 之间取值。
    * **低频组件**：对应长期记忆/知识，极其稳定。
    * **高频组件**：对应短期工作记忆，能根据当前上下文快速适应（In-context Learning）。

---

## HOPE 架构

![HOPE vs Transformer](/images/202512/nl-1.png)
*图1： 频率视角 HOPE vs Transformer*

基于嵌套学习范式和改进后的优化器，论文提出了 **HOPE** 架构，旨在统一长期记忆和短期适应能力。HOPE 架构在 Titans 架构的基础上改进而来。对于 HOPE 的每一层，有两个核心组成部分：

1.  **连续记忆系统 (Continuum Memory System, CMS)**
    * 这是一个多层级的 MLP 链条（即图1中的 **Low/Mid/High Frequency FFN**），相当于 Transformer 中的 MLP 层。
    * **多时间尺度**：不同层级的 FFN 拥有不同的 **块长度（Chunk Length）**，即不同的更新频率。
        * **高频层**（Chunk Length 小）：快速更新，负责捕捉瞬时的上下文流。
        * **低频层**（Chunk Length 大）：缓慢更新，负责沉淀长期、抽象的知识。
    * 这种设计模仿了人脑从 Gamma 波（高频）到 Delta 波（低频）的多频率记忆处理机制。

2.  **自修正 Titans (Self-Modifying Titans)**
    * 基于 Titans 架构的序列建模模块，相当于 Transformer 中的 Attention 层。
    * **上下文记忆**：Titans 中的神经记忆模块（对应图1中的 Self-Modifying Titans），通过测试时更新参数，“记忆”上下文中的关键信息，相当于 Transformer 中的 KV cache。
    * **动态 QKV**：不同于 Titans 中固定的 QKV 矩阵，HOPE 允许 QKV 根据上下文进行实时更新，并且更新频率与神经记忆模块相同。也是其名称中 Self-Modifying 的来源。

作为对比，图1中还给出了 Transformer 架构下各层参数的更新策略：

1. **FFN**： 即 Transformer 的 MLP 层。由于参数在推理时不更新，我们可以将未来所有推理视为一个 Chunk，因此 `Chunk Length = ∞, Frequency=0`；将训练时的参数变更视为一个整体（尽管训练时参数是迭代更新的，但只有最终保存的参数对推理有意义），因此训练时频率为 1。
2. **Attention**：对应 KV cache 内容，或者线性注意力的记忆矩阵，它是在线更新的，且每个 token 都在更新，因此 `Chunk Length = 1, Frequency=∞`。
3. **QKV**：对应 Attention 的 QKV 投影矩阵，它和模型的 MLP 层参数一样，在推理时不更新，仅训练时更新，因此 `Chunk Length = ∞, Frequency=0`。

---

## 后记、思考

当我们想要突破现有方法，寻找新的解法时，一个很有价值的思路是：**转换和扩展看待问题的视角，将现有方法视为更一般框架的特定场景，从而在这个“更一般框架”中寻找更好的解法。**

论文中通过转换看待梯度下降、动量、线性注意力的视角，在“频率”的框架中理解模型的不同部分，并提出了改进方法。

两个相通的例子：
- 爱因斯坦将“万有引力”视为“空间形变”，从而提出了广义相对论。
- Linux 编程哲学中，对“好品味”的描述：“有时你可以从不同角度看问题，重写它让特殊情况消失，变成正常情况。”
