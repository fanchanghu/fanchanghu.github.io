---
title: 'Nested Learning: The Illusion of Deep Learning Architectures 论文学习'
date: 2025-12-03
permalink: /posts/2025/12/nl/
tags:
  - Nested Learning
  - 嵌套学习
  - 持续学习
  - Titans
  - LLM
---

## 联想记忆 (Associative Memory)

在传统深度学习中，我们习惯于将“记忆”和“学习”视为抽象概念。本论文从神经心理学的角度出发，给出了明确的数学定义。

### 定义
给定一组由键 `K` 和值 `V` 组成的数据 `(K, V)`，联想记忆被定义为一个算子： `M: K -> V`，旨在建立键与值之间的映射。

为了从数据中学习这种映射，我们需要定义一个目标函数 `L(.)` 并求解以下优化问题：

$$
M^* = \arg \min_{M} \mathcal{L}(M(K); V) \tag{1}
$$

### 关键要素
* **键（Keys）**
* **值（Values）**
* **记忆 (Memory)**：即算子 \\(\mathcal{M}\\) 本身。
* **目标函数**：衡量键 `K` 从记忆 `M` 中召回的值 `M(K)` 与实际值 `V` 差异的函数，比如均方误差（MSE）等。
* **学习 (Learning)**：获取有效算子的过程（即最小化目标函数的过程）。

### 视角的转换
定义联想记忆的核心在于视角的转换：
- 从**训练模型**到**在线学习**：统一看待训练和推理，不仅模型本身被视为联想记忆系统，模型的训练过程（优化器）也被视为一种联想记忆系统。
- 从**参数的迭代更新问题**到**求解最小值的优化问题**：将模型中不同部分的参数更新问题，转换为求解最小值的优化问题。这就为我们以不同频率更新模型中不同部分的参数提供了可能。

下面是这种转换的具体实例：

### 单层 MLP 的梯度下降是联想记忆
以单层、无激活函数的 MLP （即线性层）训练为例，使用梯度下降更新其参数 `W` 可以被重新表述为一个优化问题。

$$
\begin{align}
W_{t+1} &= W_t - \eta_{t+1} \nabla_{W_t} \mathcal{L}^{model}(W_t;x_{t+1}) \\
        &= W_t - \eta_{t+1} \nabla_{y_t} \mathcal{L}^{model}(W_t;x_{t+1}) \otimes x_{t+1}  \tag{2}
\end{align}
$$

其中 `(x, y)` 分别对应 网络输入和输出。

\\( \nabla_{y_t} \mathcal{L}^{model}(W_t;x_{t+1}) \\) 被解释为表征空间的局部惊讶信号（Local Surprise Signal, LSS），因为它衡量了记忆系统对 `x_{t+1}` 的预测能力。

令 \\( u_{t+1}^T = \nabla_{y_t} \mathcal{L}^{model}(W_t;x_{t+1}) \\)，则上式等价于求解以下优化问题：

$$
W_{t+1} = \arg \min_{W} \left[ W x_{t+1} \otimes u_{t+1} + \frac{1}{2\eta_{t+1}}\| W - W_t \|^2  \right] \tag{3}
$$

我们对照联想记忆的关键要素，可以发现：
* **键（Keys）**： \\( x_{t+1} \\)
* **值（Values）**：\\( u_{t+1} \\)，即： \\( \nabla_{y_t} \mathcal{L}^{model}(W_t;x_{t+1}) \\)
* **记忆 (Memory)**：\\( W_t \\) 是更新前的记忆，\\( W_{t+1} \\) 是更新后的记忆
* **目标函数**： 公式（3）中 `min` 的对象。

如果将公式（3）的第二项视为正则项，那么线性层实际上是学习了一个将输入数据 `x_t` 映射到其对应的**局部惊讶信号**的映射，因此可以被视为一个联想记忆系统。

### 动量是联想记忆

当我们引入动量（Momentum）机制来改进梯度下降时，更新规则变为：

$$
\begin{align}
W_{t+1} &= W_t + m_{t+1} \\
m_{t+1} &= \alpha m_t - \eta \nabla_{W_t} \mathcal{L}(W_t; x_{t+1}) \tag{4}
\end{align}
$$

如果我们关注动量项 \\( m_{t+1} \\) 的更新公式（假设 \\( \alpha=1 \\) 以简化讨论），它实际上也是在求解一个优化问题：

$$
m_{t+1} = \arg \min_{m} \left[ -\langle m, \nabla_{W_t} \mathcal{L}(W_t; x_{t+1}) \rangle + \frac{1}{2\eta} \|m - m_t\|^2 \right] \tag{5}
$$

对照联想记忆的要素：
* **键（Key）**：\\( x_{t+1} \\)
* **值（Values）**：梯度 \\( \nabla_{W_t} \mathcal{L} \\)
* **记忆（Memory）**：动量矩阵 `m`
* **目标**：最大化动量与当前梯度的相关性（即压缩梯度信息）。

**结论**：带有动量的梯度下降实际上是一个**2级（2-level）嵌套优化系统** [cite: 174-179]。
1.  **内层（Level 2）**：优化动量矩阵 \\( m \\)，使其作为一种“元记忆（Meta-memory）”存储历史梯度的信息。
2.  **外层（Level 1）**：利用内层计算出的动量 \\( m \\) 来更新模型的主参数 \\( W \\)。

### 线性注意力是联想记忆

线性注意力（Linear Attention）机制通常具有如下递归形式：

$$
\mathcal{M}_t = \mathcal{M}_{t-1} + v_t k_t^\top \tag{6}
$$

输出为 \\( y_t = \mathcal{M}_t q_t \\)。

回顾公式（6），这完全等价于使用梯度下降（学习率为1）来优化以下目标函数：

$$
\mathcal{M}_t = \arg \min_{\mathcal{M}} \left[ -\langle \mathcal{M} k_t, v_t \rangle + \frac{1}{2} \|\mathcal{M} - \mathcal{M}_{t-1}\|^2 \right] \tag{7}
$$

对照联想记忆的要素：
* **键（Key）**：输入序列的键向量 \\( k_t \\)
* **值（Value）**：输入序列的值向量 \\( v_t \\)
* **记忆（Memory）**：注意力状态矩阵 \\( \mathcal{M} \\)
* **学习过程**：这表明线性注意力实际上是一个**在线学习过程**，它不断通过梯度下降更新内部记忆 \\( \mathcal{M} \\) 以存储上下文 `(k, v)` 对。

因此，Transformer 的推理过程本身就是一种嵌套优化：外层是预训练好的投影矩阵（Q,K,V），内层是实时优化的注意力状态。

---

## 改进联想记忆

基于“优化器即联想记忆”的视角，我们可以通过增强记忆的能力来改进优化算法：

1.  **更具表现力的关联（Expressive Association）**：
    * 普通动量是“无值（Value-less）”记忆，仅存储梯度方向。
    * **改进**：引入预处理矩阵 \\( P_i \\)（如利用 Hessian 信息），让动量学习将梯度映射到更具体的值。这解释了为何 Preconditioned SGD（如 Shampoo, Adam）更有效。

2.  **更强的目标函数（Expressive Objectives）**：
    * 普通梯度下降基于点积相似度（Hebbian Rule），记忆效率低。
    * **改进**：使用 **Delta Rule**（即 \\( L_2 \\) 回归损失 \\( \| Wx - y \|^2 \\)）作为内部优化目标。这能让记忆系统更好地管理容量，避免在新信息进入时过度抹除旧信息。

3.  **更深层的记忆结构（Deep Memory）**：
    * 普通动量是一个线性矩阵。
    * **改进**：将动量 \\( m \\) 替换为一个 **MLP 网络**。这意味着我们用一个神经网络来记忆过去的梯度模式，这被称为**深度优化器（Deep Optimizer）**。

---

## 嵌套优化 (Nested Optimization)

既然模型的各个组件（参数、优化器状态、注意力状态）本质上都在运行某种优化过程，论文提出了**嵌套学习（Nested Learning, NL）**范式，利用“频率”来组织这些过程。

### 频率的定义
对于模型中的任意组件 \\( A \\)（参数或非参数模块），其**更新频率 \\( f_A \\)** 定义为：**在单位“时间”内，该组件更新的次数**。

* **单位“时间”**：通常定义为模型中最慢的部分的更新周期，也就是说，模型中最慢的部分，其频率为 `1`。注意，这种定义下，频率是模型内的相对概念，跨模型比较频率一般没有意义。
* **层级（Levels）**：根据频率大小，我们可以将模型分解为有序的层级。频率越高，位于越底层（处理快速变化的信息）；频率越低，位于越高层（处理长期稳定的信息）。

### 视角的扩展
* **传统机器学习**是嵌套学习的一个特例：
    * **训练阶段**：所有参数频率为 1（每步都更新）。
    * **推理阶段**：所有参数频率为 0（完全冻结）。
* **Nested Learning** 的核心创新：
    * 打破二元对立，允许频率在 \\( 0 \sim \infty \\) 之间取值。
    * **低频组件**：对应长期记忆/知识，极其稳定。
    * **高频组件**：对应短期工作记忆，能根据当前上下文快速适应（In-context Learning）。

---

## HOPE 架构

基于嵌套学习范式和改进后的优化器，论文提出了 **HOPE** 架构，旨在统一长期记忆和短期适应能力。

HOPE 由两个核心部分组成：

1.  **连续记忆系统 (Continuum Memory System, CMS)**
    * 这是一个多层级的 MLP 链条。
    * **多时间尺度**：不同层级的 MLP 拥有不同的**块长度（Chunk Length）**，即不同的更新频率。
        * **高频层**（Chunk Length 小）：快速更新，负责捕捉瞬时的上下文流。
        * **低频层**（Chunk Length 大）：缓慢更新，负责沉淀长期、抽象的知识。
    * 这种设计模仿了人脑从 Gamma 波（高频）到 Delta 波（低频）的多频率记忆处理机制。

2.  **自修正 Titans (Self-Modifying Titans)**
    * HOPE 中的序列建模模块（基于 Titans 架构）具有**自指（Self-Referential）**特性。
    * **动态 QKV**：不同于 Transformer 中固定的 QKV 投影，HOPE 允许 QKV 投影矩阵根据上下文进行**实时更新**（使用基于 Delta Rule 的优化器）。
    * 这使得模型在推理阶段能够通过“修改自身”来极快地适应当前的上下文任务。
---

## 思考

当我们想要突破现有方法，寻找新的解法时，一个很有价值的思路是：**转换和扩展看待问题的视角，将现有方法视为更一般框架的特定场景，从而在这个“更一般框架”中寻找更好的解法。**

两个相通的例子：
- 爱因斯坦将“万有引力”视为“空间形变”，从而提出了广义相对论。
- Linux 编程哲学中，对“好品味”的描述：“有时你可以从不同角度看问题，重写它让特殊情况消失，变成正常情况。”
