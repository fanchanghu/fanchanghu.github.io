---
title: 'DeepSeekMath：LLM的强化学习统一范式'
date: 2025-06-28
permalink: /posts/2025/06/blog-post-2/
tags:
  - LLM
  - RL
  - GRPO
  - PPO
  - 统一范式
---

DeepSeek在[DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300v3)的讨论部分，提出了一个关于LLM中强化学习（包括SFT）的统一范式（Towards to a Unified Paradigm），对于理解LLM后训练中的各种强化学习方法非常有帮助。特记录在这里。

## 统一范式

作者提出，LLM后训练中的强化学习目标的策略梯度，可以统一用下述公式表示：

$$
\nabla_{\theta} \mathcal{J}_{\color{Red}\mathcal{A}}(\theta)=\mathbb{E}\underbrace{_{(q, o) \sim \color{Green}\mathcal{D}}}_{\text {Data Source }}\left(\frac{1}{|o|} \sum_{t=1}^{|o|} \underbrace{GC_{\color{Red}\mathcal{A}}\left(q, o, t, \color{Orange}{\pi_{rf}} \right)}_{\text {Gradient Coefficient }} \nabla_{\theta} \log \pi_{\theta}\left(o_{t} \mid q, o_{<t}\right)\right) \tag{1}
$$

其中:
* 算法 \\( \color{Red}\mathcal{A} \\) : 根据采样得到的数据和对应的奖励信号，计算梯度系数\\( GC_\mathcal{A} \\)的方法。
* 数据源 \\( \color{Green}\mathcal{D} \\) : 训练过程中，采样数据的数据分布；如：SFT的数据集、参考模型、策略模型等。
* 奖励函数 \\( \color{Orange}{\pi_{rf}} \\) : 生成奖励信号的方法，如：基于奖励模型、基于规则等。

基于这个范式，LLM后训练的各种方法，可以总结为下面的表格：

| 算法 | 数据源 | 奖励函数 | 梯度系数 |
| ---- | ---- | ---- | ---- |
| SFT | \\( q,o \sim P_{sft}(Q,O) \\) | - | 1 |
| RFT | \\( q \sim P_{sft}(Q), o \sim \pi_{sft}(O\|q) \\) | Rule | [公式（4）](#rft) |
| DPO | \\( q \sim P_{sft}(Q), o^+,o^- \sim \pi_{sft}(O\|q) \\) | Rule/Model | [公式（7）](#dpo) |
| Online RFT | \\( q \sim P_{sft}(Q), o \sim \pi_\theta(O\|q) \\) | Rule | [公式（4）](#rft) |
| PPO | \\( q \sim P_{sft}(Q), o \sim \pi_\theta(O\|q) \\) | Model | [公式（11）](#ppo) |
| GRPO | \\( q \sim P_{sft}(Q), \{o_i\} \sim \pi_\theta(O\|q) \\) | Model | [公式（15）](#grpo) |

### Supervised Fine-tuning (SFT)
SFT目标函数的梯度如下：

$$
\nabla_{\theta} \mathcal{J}_{SFT} = \mathbb{E}_{q, o \sim P_{sft}(Q, O)} \left( \frac{1}{|o|} \sum_{t=1}^{|o|} \nabla_{\theta} \log \pi_{\theta}(o_t | q, o_{<t}) \right) \tag{2}
$$

对比公式(1)，可知其梯度系数总是为1。

### Rejection Sampling Fine-tuning (RFT)
RFT目标函数的梯度如下：

$$
\nabla_{\theta} \mathcal{J}_{RFT} = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{sft}(O|q)} \left( \frac{1}{|o|} \sum_{t=1}^{|o|} \mathbb{I}(o) \nabla_{\theta} \log \pi_{\theta}(o_t | q, o_{<t}) \right) \tag{3}
$$

对比公式（1）可知，其梯度系数如下：

<a id="rft"></a>
$$
GC_{RFT}(q, o, t) = \mathbb{I}(o) =
\begin{cases}
1 & \text{the answer of } o \text{ is correct} \\
0 & \text{the answer of } o \text{ is incorrect}
\end{cases} \tag{4}
$$

### Online Rejection Sampling Fine-tuning (Online RFT)
Online SFT目标函数的梯度如下：

$$
\nabla_{\theta} \mathcal{J}_{OnRFT} = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{\theta}(O|q)} \left( \frac{1}{|o|} \sum_{t=1}^{|o|} \mathbb{I}(o) \nabla_{\theta} \log \pi_{\theta}(o_t | q, o_{<t}) \right) \tag{5}
$$

其梯度系数与RFT相同，见公式（4）。

### Direct Preference Optimization (DPO)
DPO目标函数的梯度如下：

$$
\nabla_{\theta} \mathcal{J}_{DPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), o^+, o^- \sim \pi_{sft}(O|q)} GC_{DPO}(q, o, t) \left( \frac{1}{|o^+|} \sum_{t=1}^{|o^+|} \nabla_{\theta} \log \pi_{\theta}(o_t^+ | q, o_{<t}^+) \right. \\
\left. - \frac{1}{|o^-|} \sum_{t=1}^{|o^-|} \nabla_{\theta} \log \pi_{\theta}(o_t^- | q, o_{<t}^-) \right) \tag{6}
$$

其梯度系数如下：

<a id="dpo"></a>
$$
GC_{DPO}(q, o, t) = \sigma \left( \beta \log \frac{\pi_{\theta}(o_t^- | q, o_{<t}^-)}{\pi_{\text{ref}}(o_t^- | q, o_{<t}^-)} - \beta \log \frac{\pi_{\theta}(o_t^+ | q, o_{<t}^+)}{\pi_{\text{ref}}(o_t^+ | q, o_{<t}^+)} \right) \tag{7}
$$

### Proximal Policy Optimization (PPO)
PPO目标函数如下：

$$
\mathcal{J}_{PPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{\theta_{old}}(O|q)} \frac{1}{|o|} \sum_{t=1}^{|o|} \min \left[ \frac{\pi_{\theta}(o_t | q, o_{<t})}{\pi_{\theta_{old}}(o_t | q, o_{<t})} A_t, \text{clip} \left( \frac{\pi_{\theta}(o_t | q, o_{<t})}{\pi_{\theta_{old}}(o_t | q, o_{<t})}, 1-\epsilon, 1+\epsilon \right) A_t \right] \tag{8}
$$

为了简化分析，不妨假设每个训练步骤都更新\\( \pi_{\theta_{old}} \\)，此时，显然有\\( \pi_{\theta_{old}} = \pi_{\theta}  \\)，公式（8）可以简化如下：

$$
\mathcal{J}_{PPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{\theta}(O|q)} \left[ \frac{1}{|o|}\sum_{t=1}^{|o|} A_t \right] \tag{9}
$$

其梯度如下：

$$
\nabla_{\theta} \mathcal{J}_{PPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{\theta}(O|q)} \left( \frac{1}{|o|}\sum_{t=1}^{|o|} A_t \nabla_{\theta} \log \pi_{\theta}(o_t | q, o_{<t}) \right) \tag{10}
$$

对比公式（1）可知，其梯度系数如下：

<a id="ppo"></a>
$$
𝐺𝐶_{𝑃𝑃𝑂}(𝑞, 𝑜, 𝑡, 𝜋_\theta) = 𝐴_t \tag{11}
$$

### Group Relative Policy Optimization (GRPO)
GR𝑃𝑂目标函数如下：

$$
\mathcal{J}_{GRPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)} \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left[ \frac{\pi_{\theta}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})} \hat{A}_{i,t} - \beta \left( \frac{\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta}(o_{i,t}|q, o_{i,<t})} - \log \frac{\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta}(o_{i,t}|q, o_{i,<t})} - 1 \right) \right] \tag{12}
$$

为了简化分析，与PPO一样，不妨假设\\( \pi_{\theta_{old}} = \pi_{\theta}  \\)，显然有：

$$
\mathcal{J}_{GRPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta}(O|q)} \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|}\sum_{t=1}^{|o|} \left[ \hat{A}_{i,t} - \beta \left( \frac{\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta}(o_{i,t}|q, o_{i,<t})} - \log \frac{\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta}(o_{i,t}|q, o_{i,<t})} - 1 \right)\right]  \tag{13}
$$

最终，其梯度如下：

$$
\nabla_{\theta} \mathcal{J}_{GRPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)} \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left[ \hat{A}_{i,t} + \beta \left( \frac{\pi_{ref}(o_{i,t}|o_{i,<t})}{\pi_{\theta}(o_{i,t}|o_{i,<t})} - 1 \right) \right] \nabla_{\theta} \log \pi_{\theta}(o_{i,t}|q, o_{i,<t}) \tag{14}
$$

对比公式（1）可知，其梯度系数如下：

<a id="grpo"></a>
$$
𝐺𝐶_{GR𝑃𝑂}(𝑞, 𝑜, 𝑡, 𝜋_\theta) = \hat{A}_{i,t} + \beta \left( \frac{\pi_{ref}(o_{i,t}|o_{i,<t})}{\pi_{\theta}(o_{i,t}|o_{i,<t})} - 1 \right) \tag{15}
$$
