---
title: 'DeepSeekMathï¼šLLMçš„å¼ºåŒ–å­¦ä¹ ç»Ÿä¸€èŒƒå¼'
date: 2025-06-28
permalink: /posts/2025/06/blog-post-2/
tags:
  - LLM
  - RL
  - GRPO
  - PPO
  - ç»Ÿä¸€èŒƒå¼
---

DeepSeekåœ¨[DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300v3)çš„è®¨è®ºéƒ¨åˆ†ï¼Œæå‡ºäº†ä¸€ä¸ªå…³äºLLMä¸­å¼ºåŒ–å­¦ä¹ ï¼ˆåŒ…æ‹¬SFTï¼‰çš„ç»Ÿä¸€èŒƒå¼ï¼ˆTowards to a Unified Paradigmï¼‰ï¼Œå¯¹äºç†è§£LLMåè®­ç»ƒä¸­çš„å„ç§å¼ºåŒ–å­¦ä¹ æ–¹æ³•éå¸¸æœ‰å¸®åŠ©ã€‚ç‰¹è®°å½•åœ¨è¿™é‡Œã€‚

## ç»Ÿä¸€èŒƒå¼

ä½œè€…æå‡ºï¼ŒLLMåè®­ç»ƒä¸­çš„å¼ºåŒ–å­¦ä¹ ç›®æ ‡çš„ç­–ç•¥æ¢¯åº¦ï¼Œå¯ä»¥ç»Ÿä¸€ç”¨ä¸‹è¿°å…¬å¼è¡¨ç¤ºï¼š

$$
\nabla_{\theta} \mathcal{J}_{\color{Red}\mathcal{A}}(\theta)=\mathbb{E}\underbrace{_{(q, o) \sim \color{Green}\mathcal{D}}}_{\text {Data Source }}\left(\frac{1}{|o|} \sum_{t=1}^{|o|} \underbrace{GC_{\color{Red}\mathcal{A}}\left(q, o, t, \color{Orange}{\pi_{rf}} \right)}_{\text {Gradient Coefficient }} \nabla_{\theta} \log \pi_{\theta}\left(o_{t} \mid q, o_{<t}\right)\right) \tag{1}
$$

å…¶ä¸­:
* ç®—æ³• \\( \color{Red}\mathcal{A} \\) : æ ¹æ®é‡‡æ ·å¾—åˆ°çš„æ•°æ®å’Œå¯¹åº”çš„å¥–åŠ±ä¿¡å·ï¼Œè®¡ç®—æ¢¯åº¦ç³»æ•°\\( GC_\mathcal{A} \\)çš„æ–¹æ³•ã€‚
* æ•°æ®æº \\( \color{Green}\mathcal{D} \\) : è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé‡‡æ ·æ•°æ®çš„æ•°æ®åˆ†å¸ƒï¼›å¦‚ï¼šSFTçš„æ•°æ®é›†ã€å‚è€ƒæ¨¡å‹ã€ç­–ç•¥æ¨¡å‹ç­‰ã€‚
* å¥–åŠ±å‡½æ•° \\( \color{Orange}{\pi_{rf}} \\) : ç”Ÿæˆå¥–åŠ±ä¿¡å·çš„æ–¹æ³•ï¼Œå¦‚ï¼šåŸºäºå¥–åŠ±æ¨¡å‹ã€åŸºäºè§„åˆ™ç­‰ã€‚

åŸºäºè¿™ä¸ªèŒƒå¼ï¼ŒLLMåè®­ç»ƒçš„å„ç§æ–¹æ³•ï¼Œå¯ä»¥æ€»ç»“ä¸ºä¸‹é¢çš„è¡¨æ ¼ï¼š

| ç®—æ³• | æ•°æ®æº | å¥–åŠ±å‡½æ•° | æ¢¯åº¦ç³»æ•° |
| ---- | ---- | ---- | ---- |
| SFT | \\( q,o \sim P_{sft}(Q,O) \\) | - | 1 |
| RFT | \\( q \sim P_{sft}(Q), o \sim \pi_{sft}(O\|q) \\) | Rule | [å…¬å¼ï¼ˆ4ï¼‰](#rft) |
| DPO | \\( q \sim P_{sft}(Q), o^+,o^- \sim \pi_{sft}(O\|q) \\) | Rule/Model | [å…¬å¼ï¼ˆ7ï¼‰](#dpo) |
| Online RFT | \\( q \sim P_{sft}(Q), o \sim \pi_\theta(O\|q) \\) | Rule | [å…¬å¼ï¼ˆ4ï¼‰](#rft) |
| PPO | \\( q \sim P_{sft}(Q), o \sim \pi_\theta(O\|q) \\) | Model | [å…¬å¼ï¼ˆ11ï¼‰](#ppo) |
| GRPO | \\( q \sim P_{sft}(Q), \{o_i\} \sim \pi_\theta(O\|q) \\) | Model | [å…¬å¼ï¼ˆ15ï¼‰](#grpo) |

### Supervised Fine-tuning (SFT)
SFTç›®æ ‡å‡½æ•°çš„æ¢¯åº¦å¦‚ä¸‹ï¼š

$$
\nabla_{\theta} \mathcal{J}_{SFT} = \mathbb{E}_{q, o \sim P_{sft}(Q, O)} \left( \frac{1}{|o|} \sum_{t=1}^{|o|} \nabla_{\theta} \log \pi_{\theta}(o_t | q, o_{<t}) \right) \tag{2}
$$

å¯¹æ¯”å…¬å¼(1)ï¼Œå¯çŸ¥å…¶æ¢¯åº¦ç³»æ•°æ€»æ˜¯ä¸º1ã€‚

### Rejection Sampling Fine-tuning (RFT)
RFTç›®æ ‡å‡½æ•°çš„æ¢¯åº¦å¦‚ä¸‹ï¼š

$$
\nabla_{\theta} \mathcal{J}_{RFT} = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{sft}(O|q)} \left( \frac{1}{|o|} \sum_{t=1}^{|o|} \mathbb{I}(o) \nabla_{\theta} \log \pi_{\theta}(o_t | q, o_{<t}) \right) \tag{3}
$$

å¯¹æ¯”å…¬å¼ï¼ˆ1ï¼‰å¯çŸ¥ï¼Œå…¶æ¢¯åº¦ç³»æ•°å¦‚ä¸‹ï¼š

<a id="rft"></a>
$$
GC_{RFT}(q, o, t) = \mathbb{I}(o) =
\begin{cases}
1 & \text{the answer of } o \text{ is correct} \\
0 & \text{the answer of } o \text{ is incorrect}
\end{cases} \tag{4}
$$

### Online Rejection Sampling Fine-tuning (Online RFT)
Online SFTç›®æ ‡å‡½æ•°çš„æ¢¯åº¦å¦‚ä¸‹ï¼š

$$
\nabla_{\theta} \mathcal{J}_{OnRFT} = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{\theta}(O|q)} \left( \frac{1}{|o|} \sum_{t=1}^{|o|} \mathbb{I}(o) \nabla_{\theta} \log \pi_{\theta}(o_t | q, o_{<t}) \right) \tag{5}
$$

å…¶æ¢¯åº¦ç³»æ•°ä¸RFTç›¸åŒï¼Œè§å…¬å¼ï¼ˆ4ï¼‰ã€‚

### Direct Preference Optimization (DPO)
DPOç›®æ ‡å‡½æ•°çš„æ¢¯åº¦å¦‚ä¸‹ï¼š

$$
\nabla_{\theta} \mathcal{J}_{DPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), o^+, o^- \sim \pi_{sft}(O|q)} GC_{DPO}(q, o, t) \left( \frac{1}{|o^+|} \sum_{t=1}^{|o^+|} \nabla_{\theta} \log \pi_{\theta}(o_t^+ | q, o_{<t}^+) \right. \\
\left. - \frac{1}{|o^-|} \sum_{t=1}^{|o^-|} \nabla_{\theta} \log \pi_{\theta}(o_t^- | q, o_{<t}^-) \right) \tag{6}
$$

å…¶æ¢¯åº¦ç³»æ•°å¦‚ä¸‹ï¼š

<a id="dpo"></a>
$$
GC_{DPO}(q, o, t) = \sigma \left( \beta \log \frac{\pi_{\theta}(o_t^- | q, o_{<t}^-)}{\pi_{\text{ref}}(o_t^- | q, o_{<t}^-)} - \beta \log \frac{\pi_{\theta}(o_t^+ | q, o_{<t}^+)}{\pi_{\text{ref}}(o_t^+ | q, o_{<t}^+)} \right) \tag{7}
$$

### Proximal Policy Optimization (PPO)
PPOç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼š

$$
\mathcal{J}_{PPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{\theta_{old}}(O|q)} \frac{1}{|o|} \sum_{t=1}^{|o|} \min \left[ \frac{\pi_{\theta}(o_t | q, o_{<t})}{\pi_{\theta_{old}}(o_t | q, o_{<t})} A_t, \text{clip} \left( \frac{\pi_{\theta}(o_t | q, o_{<t})}{\pi_{\theta_{old}}(o_t | q, o_{<t})}, 1-\epsilon, 1+\epsilon \right) A_t \right] \tag{8}
$$

ä¸ºäº†ç®€åŒ–åˆ†æï¼Œä¸å¦¨å‡è®¾æ¯ä¸ªè®­ç»ƒæ­¥éª¤éƒ½æ›´æ–°\\( \pi_{\theta_{old}} \\)ï¼Œæ­¤æ—¶ï¼Œæ˜¾ç„¶æœ‰\\( \pi_{\theta_{old}} = \pi_{\theta}  \\)ï¼Œå…¬å¼ï¼ˆ8ï¼‰å¯ä»¥ç®€åŒ–å¦‚ä¸‹ï¼š

$$
\mathcal{J}_{PPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{\theta}(O|q)} \left[ \frac{1}{|o|}\sum_{t=1}^{|o|} A_t \right] \tag{9}
$$

å…¶æ¢¯åº¦å¦‚ä¸‹ï¼š

$$
\nabla_{\theta} \mathcal{J}_{PPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), o \sim \pi_{\theta}(O|q)} \left( \frac{1}{|o|}\sum_{t=1}^{|o|} A_t \nabla_{\theta} \log \pi_{\theta}(o_t | q, o_{<t}) \right) \tag{10}
$$

å¯¹æ¯”å…¬å¼ï¼ˆ1ï¼‰å¯çŸ¥ï¼Œå…¶æ¢¯åº¦ç³»æ•°å¦‚ä¸‹ï¼š

<a id="ppo"></a>
$$
ğºğ¶_{ğ‘ƒğ‘ƒğ‘‚}(ğ‘, ğ‘œ, ğ‘¡, ğœ‹_\theta) = ğ´_t \tag{11}
$$

### Group Relative Policy Optimization (GRPO)
GRğ‘ƒğ‘‚ç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼š

$$
\mathcal{J}_{GRPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)} \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left[ \frac{\pi_{\theta}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta_{old}}(o_{i,t}|q, o_{i,<t})} \hat{A}_{i,t} - \beta \left( \frac{\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta}(o_{i,t}|q, o_{i,<t})} - \log \frac{\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta}(o_{i,t}|q, o_{i,<t})} - 1 \right) \right] \tag{12}
$$

ä¸ºäº†ç®€åŒ–åˆ†æï¼Œä¸PPOä¸€æ ·ï¼Œä¸å¦¨å‡è®¾\\( \pi_{\theta_{old}} = \pi_{\theta}  \\)ï¼Œæ˜¾ç„¶æœ‰ï¼š

$$
\mathcal{J}_{GRPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta}(O|q)} \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|}\sum_{t=1}^{|o|} \left[ \hat{A}_{i,t} - \beta \left( \frac{\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta}(o_{i,t}|q, o_{i,<t})} - \log \frac{\pi_{ref}(o_{i,t}|q, o_{i,<t})}{\pi_{\theta}(o_{i,t}|q, o_{i,<t})} - 1 \right)\right]  \tag{13}
$$

æœ€ç»ˆï¼Œå…¶æ¢¯åº¦å¦‚ä¸‹ï¼š

$$
\nabla_{\theta} \mathcal{J}_{GRPO}(\theta) = \mathbb{E}_{q \sim P_{sft}(Q), \{o_i\}_{i=1}^G \sim \pi_{\theta_{old}}(O|q)} \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left[ \hat{A}_{i,t} + \beta \left( \frac{\pi_{ref}(o_{i,t}|o_{i,<t})}{\pi_{\theta}(o_{i,t}|o_{i,<t})} - 1 \right) \right] \nabla_{\theta} \log \pi_{\theta}(o_{i,t}|q, o_{i,<t}) \tag{14}
$$

å¯¹æ¯”å…¬å¼ï¼ˆ1ï¼‰å¯çŸ¥ï¼Œå…¶æ¢¯åº¦ç³»æ•°å¦‚ä¸‹ï¼š

<a id="grpo"></a>
$$
ğºğ¶_{GRğ‘ƒğ‘‚}(ğ‘, ğ‘œ, ğ‘¡, ğœ‹_\theta) = \hat{A}_{i,t} + \beta \left( \frac{\pi_{ref}(o_{i,t}|o_{i,<t})}{\pi_{\theta}(o_{i,t}|o_{i,<t})} - 1 \right) \tag{15}
$$
