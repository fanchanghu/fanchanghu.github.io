---
title: 'LLM推理能力的后训练中，RL vs SFT ？'
date: 2025-03-22
permalink: /posts/2025/03/blog-post-1/
tags:
  - AI
  - LLM
  - SFT
  - RL
---

近来的一些最近进展（DeepSeek-zero等）表明，RL方法（PPO、GRPO）在推理任务上的表现，似乎优于SFT方法（DPO），why？

核心原因可能在于RL的**探索能力**。

我们知道，在RL训练中，一个重要的权衡（tradeoff）是“开发”与“探索”的平衡。而SFT主要是拟合训练数据，更多的“开发”训练数据中的模式，缺乏探索能力。只有当训练数据对真实分布的覆盖足够充分时，SFT才能取得比较好的效果。而LLM由于其庞大场景，微调数据难以做到对真实分布的充分覆盖。

基于LLM的推理任务就像一个超大“迷宫”，训练数据相当于迷宫的已知解法，SFT方法在“学习”这些解法本身（推理过程+正确答案）；而RL方法则基于“出口”（正确答案），不断探索新的解法，训练数据中的推理过程，更像是参考路径。


RL方法的缺点是**奖励稀缺**带来的训练困难。

对于LLM来说，如果基础模型能力不足或者问题难度过大，那么RL的探索过程就难以获得正确结果，从而无法有效的指导模型进行优化。

还是拿“迷宫”来类比，如果在探索时，99.99%的探索路径都是错误的，将导致RL训练过程缺乏足够的奖励信号，从而无法收敛或陷入局部优化。

对于奖励稀缺问题，可能的解决思路是“迭代训练”（DeepSeek-zero）、“课程学习”（Curr-ReFT）：先在简单问题上进行训练，提高模型能力，然后在训练数据中逐步增加困难问题。


RL的另外一个缺点是**高方差**，探索过程不可避免的引入各种噪声，从而造成优化过程难以收敛，甚至算法崩溃。

实际上，高方差是RL的通病，PO算法中各种基线（价值函数、Q函数）的引入，都是为了解决这个问题。

LLM推理任务中，困难问题的推理步骤往往包含大量token，其中的很多选择并不影响最终结果，探索这些“无关”分支将不可避免的为训练过程引入噪声。
拿“迷宫”来类比的话，相当于从起点到终点存在大量“分叉路口”，而很多“岔路”并不影响最终结果，这些“岔路”将不可避免的为探索迷宫的产生大量干扰（噪声）。


尽管如此，如果能够克服RL的缺点，那么其探索能力带来的更多可能性，则是SFT方法难以做到的。RL方法对大模型开发的潜力，应该还是很大的。