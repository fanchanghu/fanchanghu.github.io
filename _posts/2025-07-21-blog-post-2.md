---
title: 'GVPO学习'
date: 2025-07-21
permalink: /posts/2025/07/blog-post-2/
tags:
  - GVPO
  - GRPO
  - DPO
  - LLM
  - RL
---

理论分析非常精彩的一篇关于LLM RL算法的论文。作者从DPO的**策略模型隐含偏好模型**和DeepSeekMath的**LLM RL的统一范式**出发，推导并给出GVPO方法，并从多个角度，对其进行了理论分析。

论文地址：[GVPO: Group Variance Policy Optimization for Large Language Model Post-Training](https://arxiv.org/abs/2504.19599)

---

## GVPO算法的提出

### LLM后训练的统一范式

在DeepSeekMath中，DeepSeek给出了以下LLM后训练的统一形式：

$$
\nabla_{\theta} \mathcal{J}_{\color{Red}\mathcal{A}}(\theta)=\mathbb{E}\underbrace{_{(q, o) \sim \color{Green}\mathcal{D}}}_{\text {Data Source }}\left(\frac{1}{|o|} \sum_{t=1}^{|o|} \underbrace{GC_{\color{Red}\mathcal{A}}\left(q, o, t, \color{Orange}{\pi_{rf}} \right)}_{\text {Gradient Coefficient }} \nabla_{\theta} \log \pi_{\theta}\left(o_{t} \mid q, o_{<t}\right)\right) \tag{1}
$$

也可以表达成如下形式：

$$
\nabla_{\theta} \mathcal{L}(\theta) = - \sum_{(x, y_1, y_2, \ldots, y_k) \in \mathcal{D}} \sum_{i=1}^{k} w_i \nabla_{\theta} \log \pi_{\theta}(y_i | x) \tag{2}
$$

### DPO：策略模型隐含奖励模型

DPO证明了，对于如下的LLM训练目标：

$$
\max_{\pi_{\theta}} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y|x)} [R(x, y)] - \beta \mathbb{D}_{KL} [\pi_{\theta}(y|x) || \pi_{\theta'}(y|x)] \tag{3}
$$

其最优解满足：

$$
\pi^*(y|x) = \frac{1}{Z(x)} \pi_{\theta'}(y|x) e^{R(x,y)/\beta} \tag{4}
$$

其中，\\( Z(x) \\)为概率归一化的配分项。其表达式为：\\( Z(x) = \sum_y \pi_{\theta'}(y|x) e^{R(x,y)/\beta} \\)。

进一步的，对于任意策略模型\\( \pi_\theta\\)，存在如下的奖励模型\\( R_\theta\\)，使得\\( \pi_\theta\\)是公式（3）的最优解:

$$
R_\theta(x, y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{\theta'}(y|x)} + \beta \log Z_\theta(x) \tag{6}
$$

### GVPO算法

基于以上两点启发，论文作者提出，能否利用DPO的解析关系（公式6），直接设计公式（2）中的梯度系数\\( w_i \\)？

让我们首先对公式（2）做如下推理：

$$
\nabla_{\theta} \mathcal{L}(\theta) = - \sum_{(x, \{y_i\}) \in \mathcal{D}} \sum_{i=1}^{k} w_i \nabla_{\theta} \log \pi_{\theta}(y_i|x) = - \sum_{(x, \{y_i\}) \in \mathcal{D}} \sum_{i=1}^{k} w_i \nabla_{\theta} \log \frac{\pi_{\theta}(y_i|x)}{\pi_{\theta'}(y_i|x)} \tag{7}
$$

（这是因为\\( log\pi_\theta'(y|x) \\)与参数\\( \theta \\)无关，被\\( \nabla_\theta \\)作用后为0）

将公式（6）代入可得：

$$
\begin{align}
\nabla_{\theta} \mathcal{L}(\theta)
&= - \sum_{(x, \{y_i\}) \in \mathcal{D}} \sum_{i=1}^{k} w_i \nabla_{\theta} \log \frac{\pi_{\theta}(y_i|x)}{\pi_{\theta'}(y_i|x)} \\
&= - \sum_{(x, \{y_i\}) \in \mathcal{D}} \sum_{i=1}^{k} \left[ w_i \nabla_{\theta} \frac{1}{\beta}R_{\theta}(x,y_i) - w_i \nabla_{\theta} Z_{\theta}(x) \right] \\
&= - \sum_{(x, \{y_i\}) \in \mathcal{D}} \sum_{i=1}^{k} w_i \nabla_{\theta} \frac{1}{\beta}R_{\theta}(x,y_i) ||_{\sum^k_{i=1}w_i=0 \tag{8}}
\end{align}
$$

公式（8）让我们可以从隐含奖励\\( R_\theta\\)的角度，尝试寻找更好的损失函数。
并且在满足特定条件（\\( \sum^k_{i=1}w_i=0 \\)）下，其梯度部分（\\( \nabla_{\theta}R_{\theta}(x,y_i) \\)）不包含配分项\\( Z_\theta(x) \\)，只要\\( w_i \\)也不包含配分项，则对\\( \nabla_{\theta} \mathcal{L}(\theta) \\)的计算就无需考虑配分项。

也就是说，我们需要寻找满足以下条件的梯度系数\\( w_i\\) :
1. 对于任意的x，满足：\\( \sum^k_{i=1}w_i=0 \\)
2. \\( w_i \\)在使用公式（6）变为\\( \pi_\theta \\)的表达式后，可以消去配分项\\( Z_\theta(x) \\)
3. 使得损失函数\\( \mathcal{L}(\theta) \\)能够更好的体现\\( R_\theta(x,y_i) \\)与\\( R(x,y_i) \\)的差异

然后，作者给出了以下梯度系数：

$$
w_i=\beta \left[(R(x, y_i) - \overline{R(x, \{y_i\}}) - (R_{\theta}(x, y_i) - \overline{R_{\theta}(x, \{y_i\})}) \right]  \tag{9}
$$

容易验证，公式（9）满足我们对\\( w_i \\)要求的条件1和条件2。

将公式（9）代入公式（2），最终得到GVPO算法：

$$
\nabla_{\theta} \mathcal{L}_{GVPO}(\theta) = -\beta \sum_{(x, y_1, y_2, \ldots, y_k) \in \mathcal{D}} \sum_{i=1}^{k} \left[(R(x, y_i) - \overline{R(x, \{y_i\}}) - \beta(log\frac{\pi_\theta(y_i|x)}{\pi_\theta'(y_i|x)} - \overline{log\frac{\pi_\theta(y_i|x)}{\pi_\theta'(y_i|x)}})\right] \nabla_{\theta} \log \pi_{\theta}(y_i | x)
 \tag{10}
$$

下面我们继续分析，为什么说\\( \mathcal{L}_{GVPO}(\theta) \\) **能够更好的体现\\( R_\theta(x,y_i) \\)与\\( R(x,y_i) \\)的差异**。

---

## GVPO的理论分析

在开始分析前，我们先给出GVPO的策略梯度的以下等价形式：

$$
\begin{align*}
\nabla_{\theta} \mathcal{L}_{GVPO}(\theta)
& = - \sum_{x, \{y_i\}} \sum_{i=1}^{k} [(R(x, y_i) - \overline{R(x, \{y_i\}}) - (R_{\theta}(x, y_i) - \overline{R_{\theta}(x, \{y_i\}})] \nabla_{\theta} \beta \log \pi_{\theta}(y_i|x) \\
& = - \sum_{x, \{y_i\}} \sum_{i=1}^{k} [(R(x, y_i) - \overline{R(x, \{y_i\}}) - (R_{\theta}(x, y_i) - \overline{R_{\theta}(x, \{y_i\}})] \nabla_{\theta} (\overline{R_{\theta}(x, y_i)} - \overline{R_{\theta}(x, \{y_i\}}) \\
& = \frac{1}{2} \nabla_{\theta} \sum_{x, \{y_i\}} \sum_{i=1}^{k} [(R_{\theta}(x, y_i) - \overline{R_{\theta}(x, \{y_i\}}) - (R(x, y_i) - \overline{R(x, \{y_i\}})]^2 \tag{11}
\end{align*}
$$

因此，我们不妨定义如下形式，作为GVPO的理论损失：

$$
\mathcal{\hat{L}}_{GVPO}(\theta) = \mathbb{E}_{x \sim \mathcal{D}}\mathbb{E}_{y \sim \pi_s(\cdot|x)} [(R_{\theta}(x, y) - \mathbb{E}_{y \sim \pi_s}R_{\theta}(x, y)) - (R(x, y) - \mathbb{E}_{y \sim \pi_s}R(x, y))]^2 \tag{12}
$$

观察上述公式，容易发现，其正是\\( R_{\theta}(x, y) - R(x, y) \\)的方差。这正是GVPO中**Variance**一词的来源。

![Illustration comparing LoRA and LoRA](/images/202507/gvpo-1.png)
*图1: GVPO损失说明*

### GVPO目标包含了奖励最大化、参考策略靠近与策略熵最大化

为了便于分析又不失一般性，我们不妨设\\( \beta=1\\)，从公式（12）出发，可以得到以下形式：

$$
\begin{align*}
\mathcal{\hat{L}}_{GVPO}(\theta) & \overset{\nabla_{\theta}}{=} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_s(\cdot|x)} [(R_{\theta}(x, y) - \mathbb{E}_y R_{\theta}(x, y))^2 - 2(R(x, y) - \mathbb{E}_y R(x, y)) R_{\theta}(x, y)] \\
& \overset{\nabla_{\theta}}{=} \mathbb{E}_{x,y} [Var(\log \pi_{\theta}) - 2Cov(\log \pi_{\theta}, \log \pi_{\theta'}) - 2(R(x, y) - \mathbb{E}_y R(x, y)) \log \pi_{\theta}(y|x)] \\
& = -2 \mathbb{E}_{x,y} [(R(x, y) - \mathbb{E}_y R(x, y)) \log \pi_{\theta}(y|x) + Cov(\log \pi_{\theta}, \log \pi_{\theta'}) - 0.5 Var(\log \pi_{\theta})] \tag{13}
\end{align*}
$$

其中，第一步是平方差展开，并忽略关于\\( \theta \\)的常数项；
第二步是代入公式（6）并忽略关于\\( \theta \\)的常数项，注意到\\( \mathbb{E}_y[R(x, y) - \mathbb{E}_y R(x, y)]\\)作为配分项的系数等于0。

该公式的意义如图1（下面部分）所示：
* \\( (R(x, y) - \mathbb{E}_{y} R(x, y)) \log \pi_{\theta}(y|x) \\) 这一项鼓励策略为高奖励的输出分配高概率。
* \\( Cov(\log \pi_{\theta}, \log \pi_{\theta'}) \\) 这一项鼓励策略靠近参考策略。
* \\( - 0.5 Var(\log \pi_{\theta}) \\) 这一项鼓励最大化策略的熵。

### GVPO具有全局唯一的最优解\\( \pi^* \\)

如图1（中间部分）所示，GVPO损失是关于策略函数的二次型，根据凸优化理论，那么最小化GVPO损失，存在唯一的全局最优解。论文证明了该解即为\\( \pi^* \\)（可参考论文附录B.1）。

这一良好的性质，保证了使用GVPO进行优化，不会因为Loss定义而陷入局部最优解。最为对比，理论和实践都表明，DPO由于引入了BT模型对偏好建模，存在局部优化问题。

### GVPO是PG算法的推广

我们仍然假设\\( \beta=1\\)，并从公式（3）出发，进行如下推导：

$$
\begin{align*}
\nabla_{\theta} \mathcal{\hat{L}}_{PG}(\theta) &= -\nabla_{\theta}\Bigl[\mathbb{E}_{x,y\sim\pi_{\theta}(y|x)}[R(x,y)]-D_{\text{KL}}\bigl[\pi_{\theta}||\pi_{\theta'}\bigr]\Bigr] \\
&= -\nabla_{\theta}\mathbb{E}_{x}\sum_{y}\pi_{\theta}(y|x)\Bigl(R(x,y) - \log\frac{\pi_{\theta}(y|x)}{\pi_{\theta'}(y|x)}\Bigr) \\
&= -\mathbb{E}_{x}\sum_{y}\pi_{\theta}(y|x)\Bigl(R(x,y)-\log\frac{\pi_{\theta}(y|x)}{\pi_{\theta'}(y|x)}-1\Bigr)\nabla_{\theta}\log\pi_{\theta}(y|x) \\
&= -\mathbb{E}_{x,y \sim \pi_{\theta}(y|x)}\Bigl(R(x,y)-\log\frac{\pi_{\theta}(y|x)}{\pi_{\theta'}(y|x)}-1\Bigr)\nabla_{\theta}\log\pi_{\theta}(y|x) \\
&= -\mathbb{E}_{x,y \sim \pi_{\theta}(y|x)}\Bigl[R(x,y)-\log\frac{\pi_{\theta}(y|x)}{\pi_{\theta'}(y|x)}-\mathbb{E}_{y \sim \pi_{\theta}(y|x)}\Bigl(R(x,y)-\log\frac{\pi_{\theta}(y|x)}{\pi_{\theta'}(y|x)}\Bigr)\Bigr]\nabla_{\theta}\log\pi_{\theta}(y|x) \tag{14}
\end{align*}
$$

其中，第二步是使用了求导的乘法法则；
第四步是因为策略梯度的基线可以是任意当前状态（即x）的函数。注意，作为基线的 \\( \mathbb{E}_{y \sim \pi_{\theta}(y|x)}(R(x,y)-\log\frac{\pi_{\theta}(y|x)}{\pi_{\theta'}(y|x)}) \\) 这一项仅与x有关，与y无关。

另一方面，对公式（12）计算梯度，我们有：

$$
\nabla_{\theta} \mathcal{\hat{L}}_{GVPO}(\theta) = -2\mathbb{E}_{x \sim \mathcal{D},y \sim \pi_s(\cdot|x)} \Bigl[R(x, y) - \log\frac{\pi_\theta(y|x)}{\pi_{\theta'}(y|x)} - \mathbb{E}_{y \sim \pi_{s(\cdot|x)}}\bigl(R(x, y) - \log\frac{\pi_\theta(y|x)}{\pi_{\theta'}(y|x)}\bigr)\Bigr]\nabla_{\theta} \log \pi_{\theta}(y|x) \tag{15}
$$
*系数2不影响梯度方向，可以通过学习率放缩掉。*

比较公式（14）、（15），我可以得到：在公式（3）定义的目标下，GVPO是的其策略梯度算法的推广：它将采样策略从学习策略推广到更广泛的，并仍然保持相同的最优解（上一节所证明）。

这正是图1（上面部分）所展示的。

### 通过采样估计\\( \mathcal{\hat{L}}_{GVPO}(\theta) \\)是无偏且一致的

论文证明了：如果对于输入x，采样\\( k(x) \\)个样本，则\\( \mathcal{\hat{L}}_{GVPO}(\theta) \\) 的无偏且一致的估计器如下：

$$
\frac{1}{|\mathcal{D}|} \sum_{(x, \{y_i\}) \in \mathcal{D}} \frac{1}{k(x) - 1} \sum_{i=1}^{k(x)} \left[ \left( R_{\theta}(x, y_i) - \overline{R_{\theta}(x, \{y_i\})} \right) - \left( R(x, y_i) - \overline{R(x, \{y_i\})} \right) \right]^2 \tag{16}
$$

这是因为，对于任意随机变量\\( X \\)，采样k个样本，其方差的无偏一致估计为：

$$
S^2 = \frac{1}{k-1} \sum_{i=1}^k (X_i - \bar{X})^2  \tag{17}
$$

Loss估计的无偏性和一致性，对于训练过程的稳定收敛，具有很强的理论意义。

公式（16）也意味着，对于不同的输入x，如果训练时采样不同数量的响应（即\\( k(x) \\)不是常数），使用系数 \\( \frac{1}{k(x)-1} \\) 而不是常规的\\( \frac{1}{k(x)} \\) ，以消除估计偏差。

### GVPO在多轮迭代RL场景下，渐进远离初始参考模型

论文证明了：在多轮迭代场景下，每一轮都以前一轮的策略作为参考策略，那么第n轮RL的训练目标为：

$$
\mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_{\theta}(y|x)} [R(x, y)] - \frac{\beta}{n} \mathbb{D}_{KL} [\pi_{\theta}(y|x) \| \pi_{\theta_0}(y|x)] \tag{18}
$$

该目标意味着，随着迭代的进行，训练目标逐步逼近真实奖励\\( R(x, y) \\)，渐进远离初始参考模型，但仍然维持适度的靠近。

---

## 与其他算法比较

### DPO

两者都是利用公式（6）定义的解析解形式。而利用该形式有两个关键点：
- **计算效率**：由于配分项计算复杂，通常需要通过技巧规避。比如，最小化\\( R_θ(x, y) \\)的均方误差\\( \sum(R_θ(x, y) - R(x, y))^2 \\)则无法规避计算配分项。
- **与优化目标对齐**：即优化目标的解要满足：\\( R_θ(x, y) = R(x, y) \\)。比如，最小化\\( \sum(\beta\log\frac{\pi_\theta}{\pi_{\theta'}}- R(x, y))^2 \\)的解为\\( R_θ(x, y) = R(x, y) + \beta Z(x) \\)。

DPO通过对偏好建模消除配分项，而GVPO则通过\\( \sum^k_{i=1}w_i=0 \\)条件。
成对偏好建模只关注奖励的相对值，理论和实践都表明，这种方式可能导致训练目标存在多个极小值，而前面已经证明，GVPO在策略空间只有唯一的最小值。

### PPO/GRPO等PO方法

如前文分析，GVPO可以视为PG算法的推广，但具有以下优势：
- **更高的采样效率**：GVPO支持从更广泛的数据集中采样，并保持训练目标的无偏和一致，而策略梯度方法通常需要从当前策略采样，效率更低。
- **更稳定的训练**：在LLM中，对于on-policy算法，通常使用重要性采样方法，从而参考模型采样，以提高采样效率。但重要性采样系数，可能导致梯度爆炸问题，并需要进行CLIP处理。而GVPO无需重要性采样，从而避免了这个问题。

### Kimi K1.5

Kimi K1.5中也利用了公式（6）的解析解形式，它采用的正是最小化\\( R_θ(x, y) \\)的均方误差\\( \sum(R_θ(x, y) - R(x, y))^2 \\)的方法，因此需要计算配分项。其Loss如下：

$$
L(\theta) = \mathbb{E}_{(x, y^*) \sim \mathcal{D}} \left[ \mathbb{E}_{(y, z) \sim \pi_{\theta_i}} \left[ \left( r(x, y, y^*) - \beta \log Z - \beta \log \frac{\pi_{\theta}(y, z|x)}{\pi_{\theta_i}(y, z|x)} \right)^2 \right] \right]
$$

其中z为中间推理过程，y*为数据，y为策略产生的响应。

为了计算分配项，Kimi K1.5使用组内奖励的平均值\\( \overline{R(x, y_i）} \\)来估计\\( logZ(x) \\)。
