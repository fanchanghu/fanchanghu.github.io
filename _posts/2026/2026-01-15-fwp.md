---
title: '从“记忆”视角看 LLM 架构的演进'
date: 2026-01-15
permalink: /posts/2026/01/fwp/
tags:
  - FWP
  - Fast Weight Programmer
  - 快速权重程序
  - 记忆机制
  - LLM
---

传统的 Transformer 架构虽然拥有极强的信息提取能力，但其“短期记忆（KV Cache）”的存储成本随上下文长度线性增长，成为了通往“无限长度”瓶颈。为了解决这一痛点，研究界掀起了一场关于 **压缩记忆** 的架构研究。从最初将注意力机制线性化的尝试，到引入 **快速权重程序（FWP）** 思想将权重视为记忆，再到近期将 **测试时训练（TTT）** 与 **神经网络记忆（Neural Memory）**相结合。本文梳理了 LLM 记忆机制的演进逻辑，从记忆存储、更新与查询三个核心维度，剖析了从 Full-Attention 到 **Nested Learning（NL）** 的架构变迁。

## 1. 什么是 LLM “记忆”

在大型语言模型（LLM）的语境下，“记忆”是一个被频繁提及却又容易混淆的概念 —— 不同于以向量数据库的代表的 Agent 记忆， 这里，LLM 记忆指的是**模型内部与人类记忆功能类似的组件**。

* **长期记忆**：在训练阶段，模型从训练语料中“记住”的信息。
* **短期记忆**：在推理阶段，用户向模型输入的上下文信息。

本文主要聚焦于**短期记忆**视角下，LLM 架构的研究。

对于不同的模型架构，我们从以下三个方面分析记忆的机制：
1. **记忆存储**：记忆的内容以什么形态存在。
2. **记忆更新**：如何将一个新的 token 加入记忆中。
3. **记忆应用（查询）**：在生成下一个 token 时，如何从现有记忆中提取信息。

---

## 2. Full-Attention

对于使用 Full-Attention 的 LLM，用户输入的上下文被转换为 Key-Value 对并缓存起来，用以生成下一个 token，从这个意义上来说， **KV cache = 用户输入的上下文 = LLM 短期记忆**。

在 `t` 时刻， KV cache 记忆的“三要素”为：

1. **记忆存储**：\\( \mathcal{M_t} := [(k_0, v_0), (k_1, v_1), ..., (k_t, v_t)] \\)
2. **记忆更新**：\\( \mathcal{M_t} = \mathcal{M_{t-1}} \cup [(k_t, v_t)] \\)
3. **记忆查询**：\\( y_t = V_t \text{softmax}(K_t^T q_t) \\)

---

## 3. Linear-Attention

KV cache 的大小会随着用户输入的上下文长度线性增长，同时，记忆查询操作的时间也随着 KV cache 大小线性增长。从而导致长上下文场景下 LLM 内存占用增加、响应速度下降。

为了克服这一问题，一个自然的思路是压缩 KV cache 的大小 —— Linear-Attention 使用固定大小的矩阵替代 KV cache，作为 LLM 的记忆存储。

1. **记忆存储**：\\( \mathcal{M_t} \in R^{d \times d} \\)
2. **记忆更新**：\\( \mathcal{M_t} = \mathcal{M}_{t-1} + v_t \phi(k_t)^T \\)
3. **记忆查询**：\\( y_t = \alpha_t \mathcal{M}_t \phi(q_t)\\)

其中：
$$
\alpha_t = \frac{1}{\bar{k}_t^T \phi(q)}, \quad \bar{k}_t = \sum_0^t \phi(k_t)
$$

---

## 4. Delta Net

尽管 Linear-Attention 解决了“记忆”大小和查询速度线性增长问题，但是其效果往往不如 Full-Attention。论文 [Linear Transformers Are Secretly Fast Weight Programmers](https://arxiv.org/abs/2102.11174v3) 首次提出，将线性 Transformer 视为一种 **快速权重程序（Fast Weight Programmers, FWP）**，从而扩展记忆的形式。

**快速权重程序（Fast Weight Programmers, FWP）** 是一种源自 1990 年代的思想，它将模型分为两个部分：

* **慢网络（Slow Net）**：通过训练获得，参数固定，负责 **学习“如何学习”**。
* **快网络（Fast Net）**：参数在 **推理过程** 中动态变化，负责存储当前序列的特定信息。
* **工作机制**：慢网络产生“编程指令”，实时修改快网络的权重。这种“权重即记忆”的机制，为 LLM 处理无限长上下文提供了理论基础。

在 FWP 视角下，**线性 Transformer** 实际上是 FWP 的一个特例。

Linear-Attention 的记忆 `M_t` 被视为快网络，模型的其他部分被视为慢网络。在推理过程中，慢网络负责更新记忆 `M_t`，记忆 `M_t` 则被用于存储信息和后续推理。

然而，Linear-Attention 通过累加方式更新记忆 `M_t`。对于 `d` 维嵌入空间，记忆矩阵能够存储的正交向量数量不超过 `d`。

如果序列长度超过 `d` ，线性 Transformer 可能会陷入过载状态。为此，一种基于 delta 规则的 FWP 记忆方式被提出，它择性地确定要记住或忘记哪些内容。

Delta Net 记忆可以总结为如下形式：

1. **记忆存储**：\\( \mathcal{M_t} \in R^{d \times d} \\)
2. **记忆更新**：\\( \mathcal{M_t} = \mathcal{M_{t-1}} + \beta_t(v_t - \bar{v}_t) \phi(k_t)^T \\)
3. **记忆查询**：\\( y_t = \mathcal{M_t} \phi(q_t) \\)

其中：
$$
\beta_t = \sigma(W_\beta x_t), \quad \bar{v}_t = \mathcal{M_{t-1}} \phi(k_t)
$$

为了保证记忆不会因为累加而数值溢出，Delta Net 要求对核函数 `φ` 做归一化处理，即 \\( \phi(\cdot) = \frac{\phi(\cdot)}{\sum \phi(\cdot)_i} \\)。

---

## 5. Delta RNN

Delta Net 虽然引入了 Delta 规则来优化记忆存储，但在处理复杂的长程依赖时，其固定的线性更新机制仍存在局限。论文 [Linear Transformers Are Secretly Fast Weight Programmers](https://arxiv.org/abs/2106.06295) 提出了：引入**循环更新（Recurrent Update）**与**门控机制**，将 FWP 的记忆矩阵演变为一种具有类似传统 RNN 特性的高级记忆结构，即 **Delta RNN**。

在这种模式下，记忆不再只是单纯的累加或简单的 Delta 修正，而是一个动态控制的循环系统。

1. **记忆存储**：\\( \mathcal{M_t} = (W_t, R_t) \\)
2. **记忆更新**：\\( W_t = W_{t-1} + \beta_t (v_t - \bar{v}_t)k_t^T, \quad R_t = R_{t-1} + \gamma_t(v_t - \bar{v}_t)k_t^T \\)
3. **记忆查询**：\\( y_t = W_t q_t + R_t \text{softmax}(z_{t-1}) \\)

与 Delta Net 一样，`q` 和 `k` 需要为正并进行归一化处理，\\( \bar{v}_t, \beta_t, \gamma_t \\) 等参数则与 Delta Net 保持一致。

从记忆视角看，Delta RNN 通过将线性注意力的“累加性”转化为循环系统的“演化性”，获得了类似 RNN 的长期记忆能力。

理论上，任意网络都可以作为 FWP 的快网络，作者进一步提出了基于 LSTM、 MLP 等多种快网络的模型架构。

此外，[Mamba](https://arxiv.org/abs/2312.00752v2) 使用基于状态空间模型（SSM）作为快网络架构，也属于这一方法的变体。

---

## 6. Test-Time Training (TTT)

![TTT](/images/202601/fwp-1.png)
*图1： TTT 架构图*

 [TTT 架构](https://arxiv.org/abs/2407.04620) 进一步演进并重新定义了“记忆”的本质。虽然 Delta RNN 已经意识到“快网络”可以是 RNN、MLP 等复杂结构，但其权重更新仍基于慢网络生成值的累加或Delta修正。

TTT 提出了一种全新的参数更新方式：**将记忆的更新从“前向预测”转变为“测试时学习（Test-Time Training）”**。在 TTT 中，快网络的参数不再由慢网络直接更新，而是在测试时，基于特定目标直接优化快网络 —— 通过反向传播和梯度下降进行更新。慢网络则控制这个梯度下降的更新过程。

1. **记忆存储**：\\( \mathcal{M_t} = W_t = f.\text{param}() \\) ；`f` 为快网络， `W_t` 为快网络的参数。
2. **记忆更新**：\\( W_t = W_{t-1} - \eta \nabla \ell(W_{t-1}; x_t), \quad \ell(W; x_t) = \| f(\theta_K x_t; W) - \theta_V x_t \|^2 \\) ，`l` 为快网络的损失函数。
3. **记忆查询**：\\( y_t = f(\theta_Q x_t; W_t) \\)

其中，\\( \theta_K, \theta_V, \theta_Q \\) 对快网络来说属于超参数，它们由慢网络生成。显然，这种参数定义借鉴了 Transformer 中 Attention 模块的 QKV 的设计思路。

学习率 `η` 也是由慢网络生成（数据依赖的方式），具体来说是： \\( \eta(x_t)=\eta_{\text{base}} \sigma(\theta_{lr} \cdot x)  \\) ，其中基础学习率 \\( \eta_{\text{base}} \\) 对于 TTT-Linear 设置为 `1`，对于 TTT-MLP 设置为 `0.1`。

---

## 7. Titans

![MAC](/images/202511/titans-1.png)
*图2： Titans MAC 架构图*

首先，[Titans](https://arxiv.org/abs/2501.00663) 提出了一个包含长期记忆、短期记忆和持久记忆的三层记忆架构，分别对应图2中的 `Contextual Memory`, `Core`, `Persistent Memory`。
- **长期记忆 (Long-Term Memory)**：不同于我们开头定义的长期记忆，Titans 的长期记忆指的是对超出注意力窗口的 token 的记忆。其表现为一个神经记忆模块（类似 TTT），通过在推理过程中实时更新参数来“记忆”超出注意力窗口的 token，解决了传统 KV cache 随长度线性增长的瓶颈。
- **短期记忆 (Short-Term Memory)**：“记忆”注意力窗口内的 token ，提供短期的上下文依赖，采用 Full Attention 方式以最大化记忆效果。
- **持久记忆 (Persistent Memory)**：是一组 可学习的参数（Learnable Parameters/Prefix Tokens），在训练完成后固定不变，用于存储模型在预训练阶段学到的全局知识和特定任务的通用指令。

![MAC-Attn](/images/202511/titans-2.png)
*图3： Titans MAC 的注意力*

`持久记忆 Token + 长期记忆的查询结果 Token + 短期记忆的 Token` 一起构成了 Titans 中 Attention 的计算上下文。如图3所示。

此外，[Titans](https://arxiv.org/abs/2501.00663) 的神经记忆模块进一步扩展了 FWP 和 TTT 的思想。相比 TTT，它引入了 **动量（Momentum）** 和 **自适应遗忘（Adaptive Forgetting）** 机制，以更好的学习快网络参数。

1. **记忆存储**：\\( \mathcal{M_t} = W_t = f.\text{param}() \\) ；`f` 为快网络， `W_t` 为快网络的参数。
2. **记忆更新**：
   $$
   \begin{align}
   W_t &= (1-\alpha_t)\, W_{t-1} + S_t \\
   S_t &= \eta_t\, S_{t-1} - \beta_t\, \nabla \ell(W_{t-1}; x_t) \\
   \ell(W; x_t) &= \| f(\theta_K x_t; W) - \theta_V x_t \|^2
   \end{align}
   $$
3. **记忆查询**：\\( y_t = f(\theta_Q x_t; W_t) \\)

其中，\\( \theta_K, \theta_V, \theta_Q, \alpha_t, \eta_t, \beta_t \\) 是快网络（神经记忆模块）的超参数，它们由慢网络生成（数据依赖或数据无关方式）。

---

## 8. 嵌套学习（NL）

![HOPE vs Transformer](/images/202512/nl-1.png)
*图4： 频率视角 HOPE vs Transformer*

最近， [NL](https://neurips.cc/virtual/2025/loc/san-diego/poster/116123) 架构打破了传统的“模型架构 vs 优化算法”各自独立的做法，提出模型的所有组件（包括优化器如 Adam、动量项、线性层、注意力机制等）在本质上都是嵌套的、具有不同更新频率的 **优化问题**，每个优化问题都充当了一个 **关联记忆（Associative Memory）** 模块 。

其中，高频模块对应短期记忆，低频模块对应长期记忆。

1. **记忆存储**：对每个关联记忆系统，其参数即为记忆： \\( \mathcal{M_t} = W_t = f.\text{param}() \\) 。 `f` 为模型的关联记忆组件。
2. **记忆更新**：不同组件，按照自身的频率，以自身的损失函数为目标，使用梯度下降进行单步更新。以线性层为例：
  $$
  \begin{align}
  \ell(W; x_t) &= \| W^\top x_{t+1} + u_{t+1} \|^2 \\
  y'_t &= f(x_t; W_{t-1}) \\
  u_t &= \nabla_{y'_t} \ell^{model}(W_{t-1};x_t) \\
  W_t &= W_{t-1} - \eta_t \nabla \ell(W_{t-1}; x_t) \\
      &= (I - \eta_t x_t x_t^\top)W_{t-1} - \eta_t x_t u_t^\top
  \end{align}
  $$
3. **记忆查询**：\\( y_t = f(x_t; W_t) \\)

其中， \\( \ell^{model} \\) 表示整个模型的损失函数。学习率 `η` 由慢网络生成（数据相关方式）。

NL 代表了一种深刻的见解：所谓的“深度学习”其实是一种错觉，其本质是不同时间尺度下的记忆压缩过程。

---

## 总结

回顾 LLM 记忆架构的演进历程，我们可以看到一条 **从显式存储到隐式参数化** 的进化曲线。展望未来，LLM 可能不再区分“训练”与“推理”的边界。每一条新的上下文输入，都在实时地重新塑造模型的局部权重。这种“活的”记忆系统，将使模型真正具备处理无限长时序依赖的能力，并最终实现向通用人工智能（AGI）的跨越。
