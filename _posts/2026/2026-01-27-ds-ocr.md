---
title: 'DeepSeek OCR 与 DeepSeek OCR 2 学习'
date: 2026-01-27
permalink: /posts/2026/01/ds-ocr/
tags:
  - OCR
  - DeepSeek
  - DeepEncoder
  - DeepEncoder V2
  - VLM
---

[DeepSeek OCR](https://arxiv.org/abs/2510.18234) 初步探索了使用视觉模态作为 LLM 文本信息处理的有效压缩媒介。证明了视觉-文本压缩可以在不同的历史上下文阶段实现显著的token减少（7-20倍），为解决大型语言模型中的长上下文挑战提供了一个有希望的方向。
[DeepSeek OCR 2](https://github.com/deepseek-ai/DeepSeek-OCR-2/blob/main/DeepSeek_OCR2_paper.pdf) 进一步改进了 DeepSeek OCR 的编码器DeepEncoder，在此基础上提出了 DeepEncoder V2，旨在为编码器赋予因果推理能力，使其能够在基于 LLM 的内容解释之前智能地重新排序视觉 tokens，以实现 2D 图像理解。

---

# DeepSeek OCR

## 主要贡献

1. **对视觉-文本标记压缩比率进行了全面的定量分析**：其动机是：对于包含 1000 个单词的文档，至少需要多少视觉 tokens 才能进行解码？这个问题对于研究“一图胜千言”的原则具有重要意义。它证明当文本 token 数在视觉 token 数的 10 倍以内（即压缩比 < 10x）时，模型可达到约 97% 的解码（OCR）精度 。

2. **DeepEncoder 架构设计**：提出了一种串联架构，结合了局部窗口注意力和全局密集注意力，通过卷积压缩层连接，成功在处理高分辨率输入的同时保持了低激活内存（Low Activation）和极少的视觉 Token 输出 。

3. **SOTA 的端到端性能**：基于 DeepEncoder 和 DeepSeek3B-MoE 开发的模型，在 OmniDocBench 上，它仅使用 100 个视觉token就超过了 GOT-OCR2.0（256 个token/页），并且在使用少于 800 个视觉token的情况下，优于 MinerU2.0（平均每页 6000 多个token）。

## 对 OCR 编码器的要求

为了实现高效的光学上下文压缩，理想的编码器需满足以下矛盾的指标 ：

1. **高分辨率处理能力**：必须能看清密集文档的细节。
2. **低激活（Low Activation）**：在高分辨率下，中间层特征图不能过大，否则会导致显存溢出（OOM）。
3. **极简视觉 Token**：输出给 LLM 的 token 必须少，以降低推理延迟。
4. **多分辨率支持**：灵活适应从幻灯片到报纸等不同密度的文档。
5. **适中的参数量**：便于部署和训练。

## 模型架构

整体上类似 VLM：
1. 采用视觉编码器讲图片转为视觉 tokens，在本文中即为 DeepEncoder
2. 将视觉 tokens 投影到文本 Embbeding 空间
3. 利用 Transformer 解码器进行解码，计算下一个 token 的logits

### DeepEncoder

![DS OCR](/images/202601/ds-ocr-1.png)

DeepEncoder 是 DeepSeek OCR 的核心，如上图，它主要包括三个组件：一个是以窗口注意力为主的视觉感知特征提取组件，一个具有密集全局注意力的视觉知识特征提取组件，以及一个在它们之间架起桥梁的 16 倍 token 压缩器。

1. **感知组件：SAM-base (80M)**
   * **原理**：使用 SAM-base（patch-size 16），其核心是**窗口注意力（Window Attention）**机制。
   * **作用**：窗口注意力将计算限制在局部窗口内，因此即使输入  的高分辨率图像，其激活内存（Activation Memory）依然保持在较低水平，解决了高分辨率与显存占用的冲突 。

2. **压缩组件：Conv**
   * **原理**：借鉴 Vary 的设计，使用两层卷积层（Kernel 3, Stride 2, Padding 1），将通道数从 256 升至 1024 。
   * **作用**：实现 **16x 下采样**。例如，输入 `1024 * 1024` 的图像产生 `4096` 个 patch token，经过此组件压缩后变为 `4096 / 16 = 256` 个 token。这一步是“视觉 token 极少化”的关键 。

3. **知识组件：CLIP-large (300M)**
   * **原理**：使用 CLIP-large 的 Transformer 层，去除了首层 patch embedding（因为输入已是 token），核心是**密集全局注意力（Dense Global Attention）** 。
   * **作用**：由于输入 token 数已被压缩至 256 个，此时运行全局注意力计算量极小。CLIP 层负责注入通用的视觉语义知识，弥补 SAM 仅关注底层纹理的不足 。

### 多分辨率支持

![DS OCR Multi resolution support](/images/202601/ds-ocr-2.png)

DeepEncoder 支持多种输入模式，以适应不同场景 ：

* **原生分辨率（Native Resolution）**：
  * 支持四种子模式，他们对应的分辨率和 token 数量如下：
    * Tiny: 512×512 (64)
    * Small: 640×640 (100)
    * Base: 1024×1024 (256)
    * Large: 1280×1280 (400)
  * Tiny/Small：通过 Resize 调整图像，适用于 PPT 或简单文档 。
  * Base/Large：通过 Padding 保持长宽比，适用于标准文档。

* **动态分辨率（Gundam 模式）**：
  * **原理**：采用类似 InternVL2.0 的 Tiling 策略，将图像切分为 `n` 个 `640 * 640` 的局部图块（Local Views）和一个全局图块（Global View） 。
  * **Token 数**：。`n * 100 + 256` ，对于宽度和高度都小于640的图像，`n` 设置为 `0`，即Gundam模式将降级为Base模式。

动态分辨率模式与四种原生分辨率模式一起训练，以实现一个模型支持多种分辨率的目标。

### The MoE Decoder

* **功能**：解码器负责将 DeepEncoder 输出的压缩潜变量重构为文本表示。
* **架构**：采用 DeepSeek-3B-MoE (Total 3B, Active 570M) 。
* **配置**：包含 64 个路由专家，推理时激活 6 个，另有 2 个共享专家 。


## Data Engine

数据构建遵循 70% OCR 数据、20% 通用视觉数据、10% 纯文本数据的比例 。

### OCR 1.0 数据 (30M Pages)

从互联网收集的 PDF（25M 中英文，5M 其他语言）和自然场景图像。

### OCR 2.0 数据 (结构化解析)

![DS OCR chart](/images/202601/ds-ocr-3.png)

这一部分数据旨在提升模型对复杂人工图像的“深度解析（Deep Parsing）”能力。

* **图表 (Charts)**：使用 pyecharts 和 matplotlib 渲染了 1000 万张图像（折线、柱状、饼图等） 。 通过图像到 HTML 表格的转换（Image-to-HTML-table），而非简单的 JSON 提取，节省 token。
* **化学式 (Chemical Formulas)**：使用 PubChem 的 SMILES 格式数据，通过 RDKit 渲染成 500 万张图像-文本对。
* **平面几何解析 (Plane Geometry)**：使用尺寸为 `4` 的感知标尺（perception-ruler）对线段建模，生成 100 万数据。引入**几何平移不变性增强**，即在画布不同位置绘制同一几何体，对应的真值坐标系保持以几何体为中心，迫使模型学习相对几何关系而非绝对像素坐标 。

### 通用视觉与纯文本

* **通用视觉 (20%)**：包含 Image Caption、检测（Detection）、定位（Grounding）数据。虽然 DeepSeek-OCR 不是通用 VLM，但这部分数据保留了通用视觉接口，便于未来研究扩展 。
* **纯文本 (10%)**：引入 8192 长度的纯文本数据，防止模型在 OCR 训练中退化语言理解能力 。

## 实验效果与分析

![DS OCR result](/images/202601/ds-ocr-4.png)

如上图，在 `10x` 压缩比以下（即文本 token 数 `<` 10倍视觉 token 数），OCR 精度达到 **97%**，接近无损。

---

# DeepSeek OCR 2

## 主要贡献

传统的视觉-语言模型 (VLM) 在输入到 LLM 时，总是以刚性的光栅扫描顺序（从左上到右下）处理视觉 tokens，并采用固定的位置编码。然而，这与人类的视觉感知相悖，人类的视觉感知遵循 **灵活但语义连贯的扫描模式**，这种模式由内在的逻辑结构驱动：考虑追踪一个螺旋——我们的眼动遵循内在逻辑，其中每个后续注视都因果地依赖于先前的注视。

DeepSeek OCR 2 改进了 DeepEncoder 并提出了 DeepEncoder V2 —— 旨在赋予编码器因果推理能力，使其能够在基于 LLM 的内容解释之前智能地重新排序视觉 tokens，以朝着更像人类的视觉编码方向发展。

## 模型架构

![DS ORC 2 Archieve](/images/202601/ds-ocr-5.png)

如上图，相比 DeepSeek OCR V1，主要改动点是：

1. 用一个紧凑的 LLM (Qwen2 500M) 架构取代了 V1 中的 CLIP (300M) 组件，以实现视觉因果流。
   - 一方面，在大型互联网数据上训练的LLM已被证明可以有效地作为多模态模型的初始化。
   - 另一方面，Qwen2-500M 5 亿参数与 CLIP ViT（3 亿）相当，不会引入过多的计算开销。
2. 引入了可学习的查询，称为因果流 tokens，并将视觉 tokens 作为前缀预先添加。
   - 视觉 tokens 保持全局感受野，而因果流 tokens 可以获得对视觉 tokens 的 **因果排序能力**
   - 保持因果tokens和视觉tokens之间相等的基数，以便为 **再注视（re-ﬁxation）提供足够的容量
   - 只有因果流tokens对应输出（编码器输出的后半部分），被馈送到 LLM 解码器 (DeepSeek-3B) ，从而实现级联的、具有因果意识的视觉理解。

## 训练流程
DeepSeek-OCR 2 的训练分为三个阶段：（1）**编码器预训练（encoder pretraining）**，（2）**查询增强（query enhancement）**，以及（3）**解码器专门化（decoder specialization）**。

### 编码器预训练（encoder pretraining）

- 使用语言建模目标训练 DeepEncoder V2，将编码器与轻量级解码器耦合，通过下一个 token 预测进行联合优化。
- 视觉 tokenizer 从 DeepEncoder 初始化，类LLM编码器从 Qwen2-0.5B base 初始化。
- 预训练后，仅保留编码器参数用于后续阶段。
- 使用 AdamW 优化器，采用从 `1e-4` 到 `1e-6` 的余弦学习率衰减。

### 查询增强（query enhancement）

- 在 DeepEncoder V2 预训练之后，将其与 DeepSeek-3B-A500M 集成，作为最终训练和推理流程。
- 冻结视觉 tokenizer（SAM-conv），同时联合优化 LLM 编码器和 LLM 解码器，以增强查询表示。
- 使用与上一步相同的优化器和学习率衰减（从 `5e-5` 到 `1e-6`）

### 解码器专门化（decoder specialization）

- 在此阶段冻结所有 DeepEncoder V2 参数，仅更新 DeepSeek-LLM 参数。帮助 LLM 更好地理解 DeepEncoder V2 重新排序的视觉标记。
- 在此阶段执行另一次学习率衰减，从 `1e-6` 衰减到 `5e-8`

## 实验效果与分析

![DS ORC 2 Result](/images/202601/ds-ocr-6.png)

如上图， 相比 V1 ， DS OCR 2 以更少的视觉 tokens 数，实现了更好的性能。
