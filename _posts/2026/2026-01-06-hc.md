---
title: '从残差连接到流形约束的超连接'
date: 2026-01-06
permalink: /posts/2026/01/hc/
tags:
  - HC
  - Hyper-Connection
  - 超连接
  - mHC
  - Manifold-Constrained
  - 流形约束
---

本文是对 [HYPER-CONNECTIONS](https://arxiv.org/abs/2409.19606v3) 和 [mHC: Manifold-Constrained Hyper-Connections](https://arxiv.org/abs/2512.24880) 的学习报告，介绍了 **超连接（Hyper-Connections, HC）** 及其改进版 **流形约束的超连接（Manifold-Constrained HC, mHC）**。

传统的 **残差连接（Residual Connections）** 虽然解决了梯度消失，但预定义的连接强度限制了模型的表达灵活性，且易引发“表示崩溃”。**超连接（HC）** 通过在宽度方向扩展，引入了可学习的宽度连接，使网络能够根据数据动态或静态地自主寻找最优信息流路径。

**HC** 在中小规模模型上表现优异，但是当扩展至 27B 等超大规模参数时，无约束的连接矩阵会导致信号振幅爆炸，引发训练不稳定性。为了解决这个问题，**mHC** 引入了流形约束：将连接矩阵约束在 **Birkhoff 多胞形（双随机矩阵流形）** 上，从而在深层网络中恢复了残差连接的稳定性优点。

实验数据表明，mHC 在 LLM 预训练中取得更低 Loss 的同时，也表现出优秀的稳定性，说明了该架构应用潜力。

---

## 残差连接（Residual Connections）

![RC](/images/202601/hc-0.png)

传统的残差学习（ResNet）初衷是让网络学习残差映射 `f(x)`，公式表达为 \\( x^{(l+1)} = x^{(l)} + f(x^{(l)}) \\)。在这种视角下，网络被视为一系列权重的堆叠，而“跳层连接”只是为了辅助梯度流动的辅助手段。（图1左）

当我们将 **恒等连接（Identity Connection）** 视为主体时，模型的核心架构就变成了一根贯穿始终的、无损的“信息主干”。所有的权重层（Layer）不再是核心容器，而是悬挂在主干上的“侧支”。梯度能够无损地通过 \\( x^{(l+1)} \to x^{(l)} \\) 的直接路径。这种视角有助于我们设计极深的网络，因为它强调了保持原始输入特征的重要性，而非一味追求非线性变换。

### 残差连接的成功与局限性

当我们把恒等连接看作主干，把权重层看作调节分支，就可以发现，目前的架构存在以下限制：
1. **固定的融合比例**： 在标准的残差连接中，主干信息和残差信息的比例默认是 `1:1`（即直接相加）。这种静态的权重分配忽略了不同任务、不同数据样本对“原始特征”与“新特征”需求量的差异。
2. **缺乏动态适应性**： 某些层可能需要保留大量的原始上下文（高强度恒等连接），而另一些层可能需要完全由新学到的变换主导（低强度恒等连接）。预定义的、不可学习的连接强度强制所有层遵循相同的融合逻辑，压制了模型在不同深度下自适应调整特征流的能力。
3. **表示空间的坍缩**： 由于连接方式是预设的（Pre-defined），模型无法在训练过程中优化“信息流”本身的拓扑结构。这限制了模型在处理复杂非线性关系时的灵活性，导致其只能在给定的、受限的子空间内进行表达。
4. **梯度消失和表示坍塌的权衡**： 残差连接存在两个主要变体：前置归一化（Pre-Norm）和后置归一化（Post-Norm）。
   - 前置归一化在每个残差块之前对输入应用归一化操作，可以解决梯度消失问题，但它也可能导致深度表示中出现坍塌问题，即更深层的隐藏特征变得高度相似，从而减少了额外层数的贡献。
   - 后置归一化在每个残差块的输出之后应用归一化，降低了隐藏状态对后续层的影响，缓解了表示坍塌的问题，但归一化操作通常具有收缩的趋势，使得网络重新引入了梯度消失的问题。

超连接的提出，正是基于对以上问题的研究和改进。

---

## 超连接（Hyper-Connections, HC）

本部分基于论文 [HYPER-CONNECTIONS](https://arxiv.org/abs/2409.19606v3) 。

### 超连接的网络结构

![HC](/images/202601/hc-1.png)
*图1：n=2时的 HC 示意图*

HC 通过将隐藏向量扩展为 `n` 个副本，引入了更复杂的拓扑结构（参考`图1b`）：
1. 设原网络中，隐藏向量 \\( h^{(l-1)} \in R^d \\) 作为第 `l` 层的输入，网络的初始输入为 \\( h^{(0)} \\)。
2. 在 HC 网络中，\\( h^{(0)} \\) 被复制 `n` 次以形成初始输入矩阵 \\( H^{(0)}= (h^{(0)}, ..., h^{(0)})\in R^{n \times d} \\) 。其中 `n` 是扩展率。后续第 `l` 层的输入： \\( H^{(l-1)} = (h_1^{(l-1)}, ..., h_n^{(l-1)}) \in R^{n \times d} \\) 。为了表示方便，后续我们省略层数 `l`。
3. `图1b` 为 `n=2` 时 HC 的网络结构：
   - 输入 \\( H=(h_1, h_2) \in R^{2 \times d} \\)
   - 计算残差块的输出： \\( \bar{h} = \mathcal{T}(H^TA_m) \in R^d \\) ，其中 \\(\mathcal{T}\\) 对应图中的 layer ， \\( A_m = (\alpha_{1,0}, \alpha_{2,0}) \in R^2 \\) ，即对输入的加权求和系数。
   - 计算整个 HC 层的输出：\\( \hat{H} = B \bar{h}^T + A_r^T H \in R^{2 \times d} \\) ，其中 \\(B = (\beta_1, \beta_2) \in R^2, A_r = \begin{bmatrix} \alpha_{1,1}, \alpha_{1,2}\\\alpha_{2,1}, \alpha_{2,2} \end{bmatrix} \\)
4. **深度连接（Depth-connections）**：从 `图1c` 可以看出， HC 可以视作是残差连接的泛化形式，它在不同深度的层以及残差块之间分配连接权重。
5. **宽度连接（Width-connections）**：从 `图1d` 可以看出，HC 允许在同一层内的多个隐藏向量之间进行侧向信息交换，实现了横向信息的整合。

### 静态超连接与动态超连接

HC 提供了两种灵活的实现方式：

* **静态超连接（SHC）**：连接权重（如图1b中的`α, β`）是可学习的标量，在训练完成后固定不变。对应如下定义：
  $$
  \mathcal{HC} = \begin{bmatrix}
    \mathbf{0_{1 \times 1}}, &\mathbf{B}^T \\
    \mathbf{A_m}, &\mathbf{A_r}
  \end{bmatrix}
  $$

* **动态超连接（DHC）**：连接权重由网络根据输入数据实时预测，允许模型根据不同样本动态调整连接强度。对应如下定义：
  $$
  \mathcal{HC}(\mathbf{H}) = \begin{bmatrix}
    \mathbf{0_{1 \times 1}}, &\mathcal{B}(\mathbf{H})^T \\
    \mathcal{A_m}(\mathbf{H}), &\mathcal{A_r}(\mathbf{H})
  \end{bmatrix}
  $$

  其中：

  $$
  \begin{align}
  \bar{\mathbf{H}} &= \text{RMSNorm}(\mathbf{H}) \\
  \mathcal{B}(\mathbf{H}) &= s_\beta \cdot \tanh(\bar{\mathbf{H}}\mathbf{W}_\beta) + \mathbf{B} \in R^{n \times 1} \\
  \mathcal{A_m}(\mathbf{H}) &= s_\alpha \cdot \tanh(\bar{\mathbf{H}}\mathbf{W}_m) + \mathbf{A}_m \in R^{n \times 1}\\
  \mathcal{A_r}(\mathbf{H}) &= s_\alpha \cdot \tanh(\bar{\mathbf{H}}\mathbf{W}_r) + \mathbf{A}_r \in R^{n \times n}
  \end{align}
  $$

### 超连接的优势

HC 的优势在于其自适应性。理论上，它允许网络在训练过程中自动寻找最优的残差强度，并能根据需要将层排列为串联或并联结构。这种设计不仅克服了传统残差连接的刚性限制，且在计算量和参数量增加极小的情况下显著提升了表达能力。

### 实验结果
![HC Training Loss vs Tokens](/images/202601/hc-2.png)
*图2：训练 Loss vs 训练 Tokens*

在 1B 的大型语言模型（LLM）的预训练实验中，HC 在稠密和稀疏模型上均表现出优于传统残差连接的性能。图2 的结果证实这一点。并且，作者指出这种性能提升在视觉任务中同样适用，证明了 HC 结构的通用性。

---

## 流形约束的超连接（mHC）

本部分基于论文 [mHC: Manifold-Constrained Hyper-Connections](https://arxiv.org/abs/2512.24880)

### 普通超连接的问题

普通 HC 在小规模实验中表现优异，但在扩展到大规模（如 27B 参数）训练时会出现**训练不稳定性**。原因在于无约束的连接矩阵破坏了残差连接固有的“恒等映射”特性，导致信号在深层网络传播时振幅不受控，引发梯度爆炸和损失突刺。

### 符号说明

本论文中，作者重新定义了 HC 的一些数学符号：

$$
\begin{align}
x_l &:= H^{l} = (h_1^{(l)}, ..., h_n^{(l)}) \in R^{n \times d} \\
\mathcal{F} &:= \mathcal{T} \\
H^{res} &:= \mathcal{A_r}(H) \\
H^{pre} &:= \mathcal{A_m}(H) \\
H^{post} &:= \mathcal{B}(H) \\
\end{align}
$$

这样，DHC 可以表示为如下公式：
$$
x_{l+1} = H_l^{res} x_l + H_l^{post} \mathcal{F}((H_l^{pre})^T x_l, W_l) \tag{1}
$$

### 流形约束

![mHC](/images/202601/hc-3.png)
*图3：mHC 架构示意图*

为了修复稳定性问题，mHC 将 HC 的连接矩阵投影到 **双随机矩阵流形（Birkhoff 多胞形）** 上，如 `图1c`。

mHC 的数学表示如下：

$$
x_{l+1} = P_{M^{res}}(H_l^{res}) x_l + P_{M^{post}}(H_l^{post}) \mathcal{F}\left(P_{M^{pre}}(H_l^{pre})^T x_l, W_l\right) \tag{2}
$$

其中，连接矩阵采用动态超连接方式计算，详情如下：

$$
\begin{align}
x_l' &= \text{RMSNorm}(\text{flatten}(x_l)) \\
H_l^{pre} &= \alpha_l^{pre} \cdot (x_l' \phi_l^{pre}) + \mathbf{b}_l^{pre} \\
H_l^{post} &= \alpha_l^{post} \cdot (x_l' \phi_l^{post}) + \mathbf{b}_l^{post} \\
H_l^{res} &= \alpha_l^{res} \cdot \text{mat}(x_l' \phi_l^{res}) + \mathbf{b}_l^{res} \\
P_{M^{pre}} &:= \sigma(\cdot) \\
P_{M^{post}} &:= 2\sigma(\cdot) \\
P_{M^{res}} &:= \text{Sinkhorn-Knopp}(\cdot)
\end{align}
$$

双随机矩阵是所有元素非负，且每行每列之和均为 `1`的矩阵。**Birkhoff 多胞形** 是全体双随机矩阵的集合。它具有如下优点：
1. **范数保持**：双随机矩阵的谱范数有界（`<=1`），有效抑制了梯度爆炸。
2. **恢复恒等映射**：观察公式（2），由于 \\(P_{M^{res}}(H_l^{res})\\) 的每一行和列的和为 `1`，对每个流，其混合的上一层的各流的权重之和为`1`，达到了和残差连接的恒等映射相同的作用。
3. **组合封闭性**：该流形对矩阵乘法封闭，确保了多层叠加后的复合映射依然稳定，从而在深层网络中也能保持恒等映射的优良特性。
4. **Birkhoff 多胞形的几何解释**：Birkhoff 多胞形是置换矩阵集合的凸包。这提供了一个清晰的几何解释：残差映射充当置换的凸组合。数学上，重复应用这样的矩阵往往会增加跨流单调地混合信息，有效地发挥了特征融合机制的作用。

### 实验结果

![mHC vs HC](/images/202601/hc-4.png)
*图4：mHC vs HC 训练效果*

如图，在 27B 规模下，mHC 比 HC 取得了更低的 Loss 表现，并且损失曲线和梯度范数比普通 HC 更加平滑。
