---
title: 'Engram: Conditional Memory via Scalable Lookup 论文学习'
date: 2026-01-17
permalink: /posts/2026/01/engram/
tags:
  - Engram
  - Conditional Memory
  - 条件记忆
  - Deepseek
  - LLM
---

本文是对 [Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models](https://arxiv.org/abs/2601.07372) 的学习报告，介绍了 deepseek 最近提出的一种条件记忆架构：**Engram**。Engram 是一个将经典 N-gram 嵌入 LLM 的模块，通过 `O(1)` 的哈希查找，解除了骨干网络早期层对静态知识重建的负担，让模型聚焦复杂的推理计算。此外，通过在 Engram 和 MoE 之间进行 **稀疏性分配（Sparsity Allocation）**，作者验证了条件记忆可以作为一种与 MoE 互补的稀疏性维度。

---

## 问题背景

“稀疏性是智能系统的一个反复出现的设计原则”，在目前的 LLM 中，这一原则主要通过混合专家（MoE）实现，它通过条件计算来扩展容量。

论文提出，语言信号固有的异质性表明在结构优化方面仍有很大的空间。具体而言，语言建模包含两个性质上不同的子任务：组合推理和知识检索。虽然前者需要深入的动态计算，但文本的很大一部分——例如命名实体和公式化模式——是局部的、静态的和高度刻板的。

经典 N-gram 模型在捕获此类局部依赖关系方面有效且成本低廉。而当前的 LLM 不得不通过计算来模拟此类检索。这个过程本质上相当于对静态查找表进行昂贵的运行时重建，将宝贵的序列深度浪费在琐碎的操作上，而这些深度原本可以分配给更高层次的推理。

## 什么是 N-gram

在自然语言处理中，**N-gram** 是一种基于连续序列的统计模型，其核心思想是“局部相关性”：即预测一个词的出现概率时，只考虑其前面的  个词。

* **基本定义**：N-gram 是给定文本中  个连续项（可以是字符、音节或词）的序列。
* **Unigram (n=1)**: `Alexander`, `the`, `Great`
* **Bigram (n=2)**: `Alexander the`, `the Great`
* **Trigram (n=3)**: `Alexander the Great`

## 为什么 Engram 要引入 N-gram？

在当前的 Transformer 架构中，模型对知识的获取主要依赖于两种方式：**参数化存储**（通过 MLP 层背诵事实）和 **上下文注意力**（通过 Attention 检索 KV Cache）。然而，这两种方式在处理“事实性、模式化”知识时存在缺陷：

1. **计算冗余**：对于像 `Alexander the Great` 或 `E=mc²` 这样高度刻板的模式，Transformer 仍需通过多层注意力机制和非线性映射在运行时重新“合成”这些语义。这本质上是用昂贵的**动态计算**去模拟简单的**静态查找**。
2. **早期层的深度浪费**：研究表明，底层 Transformer 块往往承担了大量的“特征重建”任务。如果能提供一种直接的查找原语（Primitive），就能释放这些层的计算资源，让其专注于更高阶的逻辑推理。

## Engram 架构

![Engram](/images/202601/engram-1.png)
*图1：Engram 架构*

如图，Engram 在传统 Transformer 块的注意力层前添加了一个 Engram 模块，旨在通过在结构上将静态模式存储与动态计算分离来增强Transformer主干。

给定一个输入序列 \\( X = (x_1, ..., x_T) \\) 和在第 `ℓ` 层隐藏状态 \\( H^{(l)} \in R^{T \times d} \\) ，Engram 对每个位置 `t` 执行以下两个操作：**检索** 和 **融合**。

### 基于哈希 N-gram 的稀疏检索

该过程包括两个阶段：**分词压缩** 和 **多头哈希**。

- **分词压缩**：N-gram 模型通常直接在分词器输出上运行，但 Engram 的目标并非无损重建。为了最大化语义密度，首先需要合并词表中类似的 token，比如 `Apple` 与 `_apple` 。此过程最终使 `128k` 分词器的有效词汇量减少了 `23%` 。

- **多头哈希**：直接参数化所有可能的 N-gram 的组合空间是难以处理的（N-gram 的组合数量为词表大小的 `N` 次方）。因此需要采用基于哈希的方法，为了减轻冲突，为每个N-gram 阶数 `n` 采用 `K` 个不同的哈希头。

在实验中，作者采用 2-gram 和 3-gram 两种编码，对每个 gram ，采用 8 个哈希头。

以`图1`为例，
1. 当前 token `x_t` 为 `Great` ， 2-gram 为 `the Great` ， 3-gram 为 `Alexander the Great`
2. 对每个 gram ，分别定义 8 个哈希方法，记为： \\( \phi_{n, k}, \quad n \in [2,3], k \in [1,8] \\)
3. 计算出所有哈希值： \\( z_{t,n,k} = \phi_{n,k}(\text{gram}^{(n)}(x_t)), \quad n \in [2,3], k \in [1,8] \\) ， `t` 为当前 token 索引， \\(\text{gram}^{(n)}\\) 为 n-gram 。
4. 对每个哈希值，查询查询表 `E`（共16个哈希表） ，得到对应嵌入向量 `e`(共16个嵌入向量) ： \\( e_{t,n,k} = E_{n,k}[z_{t,n,k}] \\)
5. 将上述 `16` 个嵌入向量拼接，得到 token `x_t` 对应的 Engram 编码向量: \\( e_t = \|_{n=2}^3 \|_{k=1}^8 e_{t,n,k} \\)

上述过程对应了`图1右`的下半部分（到 `Concat` 组件）。

## 上下文感知门控与融合

![Engram 2](/images/202601/engram-2.png)
*图1：Engram 训练与并行推理*

由于上述嵌入向量 `e_t` 是上下文无关的，缺乏上下文适应性，并且可能因哈希冲突或多义性而受到影响。在将上述数据融合到主干网络前，作者增加了一种受注意力机制启发的上下文感知门控机制。

具体来说：
1. 将 `e_t` 视作记忆，进行键和值投影： \\( k_t = W_K e_t, v_t = W_V e_t \\)
2. 利用当前的隐藏状态 `h_t` 作为查询向量，计算注意力系数： \\( \alpha_t = \sigma \left( \frac{\text{RMSNorm}(h_t)^\top\text{RMSNorm}(k_t)}{\sqrt{d}} \right) \\)
3. 门控输出为： \\( \tilde{v}_t = \alpha_t \cdot v_t \\)
4. 引入 1 维卷积和非线性： \\( Y = SiLU(Conv1d(RMSNorm(\tilde{V}))) + \tilde{V} \\) ，其中 \\( \tilde{V} = R^{t \times d} \\) 为上述门控值的序列。
5. 最后，通过残差连接，将上述结果集成到主干网络 \\( H^{(\ell)} = H^{(\ell)} + Y \\) 。注意，并非每一层都需要添加 Engram，只在特定层添加即可，并且可以充分利用GPU和CPU的并行计算能力。（如`图2b`）

上述过程对应了`图1右`的上半部分（从 `Concat` 组件开始）。

## 稀疏性分配

### Engram 的稀疏性定义
在前向阶段，仅当前上下文中的 token 用于计算 2-gram 和 3-gram ，并进行后续的哈希查询，被检索到的嵌入向量才会“激活”，嵌入表中的其他向量都是不参与前向过程，因此是未激活的。

### 稀疏分配
当总参数和训练计算量固定时（等参数和等 FLOPs），将稀疏容量（sparse capacity）在 MoE 和 Engram 之间分配。称之为 **稀疏分配**。

分配比例 `ρ ∈ [0, 1]` 定义为分配给 MoE 专家容量的非激活参数预算的比例。
- `ρ = 1` 对应于一个纯粹的 MoE 模型（所有非活跃参数都被路由到专家）。
- `ρ < 1` 减少了路由专家的数量，并将释放的参数重新分配给 Engram 嵌入槽。

![Sparsity Allocation Result](/images/202601/engram-3.png)
*图3：稀疏分配结果*

如`图3左`，可以发现，验证损失与分配比例 `𝜌` 之间存在 `U` 形关系。

最优的分配比例大约位于 `ρ ≈ 75%–80%` 之间。

### 无限内存机制评估

考虑这样一个问题：**因为 Engram 的查找成本是 O(1) 开销的，如果内存预算放宽或积极扩展，Engram 本身会表现出什么样的扩展行为？**

`图3右` 的结果表明，扩展 Engram 内存槽的数量可以明显且持续地改善验证损失。在所探索的范围内，曲线遵循严格的幂律（在对数空间中呈线性），表明 Engram 提供了一个全新的缩放维度：**更大的内存持续带来收益，而无需额外的计算。**

> 注：扩展 Engram 内存即扩大哈希表的最大槽位数量。比如，最大槽位数量从 `10^6` 增加到 `10^7` ，意味着哈希表的内存占用增加 10 倍，同时哈希冲突的概率将显著降低。
